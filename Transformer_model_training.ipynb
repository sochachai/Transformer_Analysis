{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7AOFLlbAkA779FAnE6AN1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sochachai/Transformer_Analysis/blob/main/Transformer_model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load packages"
      ],
      "metadata": {
        "id": "QadEcdM9HWsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check directory\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8gwuRviI0JD",
        "outputId": "005935d8-ec1b-472e-c867-bd80395c7e01"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# confirm the assistant py file has been uploaded to the correct directory\n",
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kj093MmNI24A",
        "outputId": "e4df2ba6-bc0d-4383-f10f-690cfc606572"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my_transformer_utils.py  \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check Python version\n",
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9lyQC3sJbJs",
        "outputId": "118424bb-3c80-440f-9466-110f7d72df18"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install torch\n",
        "!pip install torch==2.2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRb0L4jOJzuY",
        "outputId": "9fee20c8-1e07-4ee8-f80e-b473e0aa847d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==2.2.0 in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0) (12.3.101)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.0) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check torch version\n",
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AWYB4ZIJnlB",
        "outputId": "297dee6e-e9f7-4d36-9ba5-f0084b958466"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZL5Plt3mvusS"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "# Issue caused by torch version described below:\n",
        "# the original package should be pyitcast.transformer_utils\n",
        "# the problem is that pyitcast.transformer_utils relies on an old version of torch (1.3.1 should work)\n",
        "# and causes the error \"IndexError: invalid index of a 0-dim tensor. Use `tensor.item()` in Python...\"\n",
        "# but the installation of an old version of torch that match pyitcast.transformer is not trivial\n",
        "# versions of 1.11.0 or after is not compatible with pyitcast.transformer and installation of them will not\n",
        "# solve the issue\n",
        "# To solve this issue:\n",
        "# download the pyitcast.transformer_utils (open by clicking the error message) as a py file\n",
        "# modify the last line of the SimpleLossCompute class from \"return loss.data[0] * norm\"\n",
        "# to \"return loss.data * norm\"\n",
        "\n",
        "from my_transformer_utils import Batch\n",
        "from my_transformer_utils import run_epoch\n",
        "from my_transformer_utils import greedy_decode\n",
        "from my_transformer_utils import get_std_opt # get_std_opt is based on Adam optimizer\n",
        "from my_transformer_utils import LabelSmoothing # offset human label errors to prevent overfitting\n",
        "from my_transformer_utils import SimpleLossCompute # calculate loss after smoothing, use cross_entropy_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct the class of Embeddings and Positional Encoding."
      ],
      "metadata": {
        "id": "UBTCCLP4Hc3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        '''\n",
        "        :param d_model: embedding dimension\n",
        "        :param vocab: size of vocabulary\n",
        "        '''\n",
        "        # Initialization\n",
        "        super(Embeddings, self).__init__()\n",
        "        # Defrine a word embedding object\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        # Instantiate d_model\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        :param x: tensor representing the original text\n",
        "        '''\n",
        "        return self.lut(x) * math.sqrt(self.d_model)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout, max_len = 5000):\n",
        "        '''\n",
        "        :param d_model: dimension of the encoding\n",
        "        :param dropout: dropout rate from 0 to 1\n",
        "        :param max_len: the maximum length of a sentence\n",
        "        '''\n",
        "        # Inherit the initialization of nn.Module\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # Objectify dropout\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Inherit a positional encoder matrix, max_len * d_model\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        # Inherit an absolute position matrix, max_len * 1\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "\n",
        "        # Define the conversion matrix, initialization with gap = 2\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0)/d_model))\n",
        "\n",
        "        # Copy the absolute position matrix to the positional encoder matrix\n",
        "        # by sine and cosine wave according to the parity of column indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) # even indiced columns are imputed by sine\n",
        "        pe[:, 1::2] = torch.cos(position * div_term) # odd indiced columns are imputed by cosine\n",
        "\n",
        "        # Extend pe to 3-dimensional tensor\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # Register pe to a buffer, the buffer is not a parameter of the class\n",
        "        # the buffer will not be updated along with the model update\n",
        "        # but it can be loaded along with the model\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        :param x: Tensor of text\n",
        "        :return: x + the positional encoding\n",
        "        '''\n",
        "        # Shrink the size of pe to save storage\n",
        "        # by converting the second dimension, i.e. the dimension of max_len\n",
        "        # to the size of the sentence len of x, i.e. the second dimension of x\n",
        "        x = x + Variable(self.pe[:,:x.size(1)], requires_grad = False) # False: pe will not be updated\n",
        "        return self.dropout(x)\n",
        "\n",
        "def attention(query, key, value, mask = None, dropout = None):\n",
        "    '''\n",
        "    :param query: vectorized original text\n",
        "    :param key: key words of text\n",
        "    :param value: the original value of key, summarization of query\n",
        "    :param mask: hide words to avoid data leakage\n",
        "    :param dropout: dropout rate of neural network\n",
        "    :return:\n",
        "    '''\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9) # compare each position with 0\n",
        "\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "\n",
        "    return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "def clones(module, N):\n",
        "    '''\n",
        "    :param module: one attention layer\n",
        "    :param N: the number of module\n",
        "    '''\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)]) # deepcopy uses a different memory\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, head, embedding_dim, dropout = 0.1):\n",
        "        '''\n",
        "        :param head: the number of heads\n",
        "        :param embedding_dim: the embedding dimension\n",
        "        :param dropout: default dropout rate set to 0.1\n",
        "        '''\n",
        "        # Inherit the initialization\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        # head must be an integral factor of embedding_dim\n",
        "        assert embedding_dim % head == 0\n",
        "        # each head is assigned with the following dimension\n",
        "        self.d_k = embedding_dim // head # division in the integral domain Z\n",
        "        # substantiate head\n",
        "        self.head = head\n",
        "        # create linear layers, we need 4 of them for Q, K, V and the final connection\n",
        "        self.linears = clones(nn.Linear(embedding_dim, embedding_dim), 4)\n",
        "        # create the attention and dropout rate\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p = dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # We have 4 linears and only zipping the first 3 with Q, K, V, the last linear is not used.\n",
        "        # view(batch_size, -1, self.head, self.d_k).transpose(1,2)\n",
        "        # is not the same with view(batch_size, self.head, -1, self.d_k)\n",
        "        # self.head and self.d_k should be neighboring to get embedding_dim in order for the tensor\n",
        "        # to interpret the relationship between the meaning of words of their positions in a sentence\n",
        "\n",
        "        query, key, value = \\\n",
        "            [model(x).view(batch_size, -1, self.head, self.d_k).transpose(1,2)\n",
        "             for model, x in zip(self.linears, (query, key, value))]\n",
        "\n",
        "        x, self.attn = attention(query, key, value, mask = mask, dropout = self.dropout)\n",
        "\n",
        "        # Reshape x\n",
        "        # must use contiguous method after the transpose before the view method\n",
        "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.head * self.d_k)\n",
        "\n",
        "        # Pass x to the 4th linear layer\n",
        "        return self.linears[-1](x)\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        '''\n",
        "        :param d_model: embedding dimension\n",
        "        :param d_ff: transitional dimension\n",
        "        :param dropout: default dropout set to 0.1\n",
        "        '''\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w1 = nn.Linear(d_model, d_ff)\n",
        "        self.w2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w2(self.dropout(F.relu(self.w1(x))))\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, features, eps = 1e-6):\n",
        "        '''\n",
        "        :param features: embedding dimension\n",
        "        :param eps: avoid zero denominator\n",
        "        '''\n",
        "        super(LayerNorm, self).__init__()\n",
        "        # nn.Parameter formats the variables to intrinsic parameters\n",
        "        # they will be updated along with the model in contrast with buffer\n",
        "        self.a2 = nn.Parameter(torch.ones(features))\n",
        "        self.b2 = nn.Parameter(torch.zeros(features))\n",
        "        # initialization of eps\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        :param x: the output of previous layer\n",
        "        :return: numerical standardized x\n",
        "        '''\n",
        "        mean = x.mean(-1, keepdim = True)\n",
        "        std = x.std(-1, keepdim = True)\n",
        "        return self.a2 * (x - mean) / (std + self.eps) + self.b2\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "    def __init__(self, size, dropout = 0.1):\n",
        "        '''\n",
        "        :param size: embedding size\n",
        "        :param dropout: deactivation of neurons to avoid overfitting\n",
        "        '''\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        '''\n",
        "        :param x: the output of previous layer\n",
        "        :param sublayer: a function, e.g. Multihead_Attention, PositionwiseFeedForward etc.\n",
        "        :return: x plus sublayer functioning on normed x with dropout\n",
        "        '''\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        '''\n",
        "        :param size: embedding dimension\n",
        "        :param self_attn: attention\n",
        "        :param feed_forward: positionwise feed forward\n",
        "        :param dropout: avoid overfitting\n",
        "        '''\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        '''\n",
        "        :param x: output of previous layer\n",
        "        :param mask: tensor mask to prevent data leakage\n",
        "        '''\n",
        "        # input matrix followed by operating function, returns an output matrix\n",
        "        # sublayer(x, function) will return function(x)\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) # the forward function of multihead attention\n",
        "        return self.sublayer[1](x, self.feed_forward) # the forward function of pointwise feedforward\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    # Encoder is a collection of Encoder Layers\n",
        "    def __init__(self, layer, N):\n",
        "        '''\n",
        "        :param layer: one encoder layer\n",
        "        :param N: the number of encoder layers\n",
        "        '''\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers: x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        '''\n",
        "        :param size: embedding dimension\n",
        "        :param self_attn: masked multihead attention\n",
        "        :param src_attn: multihead attention\n",
        "        :param feed_forward: pointwise feed forward\n",
        "        :param dropout: avoid overfitting\n",
        "        '''\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        # 3 sublayer for a decoder layer\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        "\n",
        "    def forward(self, x, memory, source_mask, target_mask):\n",
        "        '''\n",
        "        :param x: output of previous layer\n",
        "        :param memory: result of encoder\n",
        "        :param source_mask: delete unnecessary info to improve model performance\n",
        "        :param target_mask: hide info to prevent data leakage\n",
        "        :return: output tensor\n",
        "        '''\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, target_mask))\n",
        "        # for second layer, Q = x, K = V = m\n",
        "        x = self.sublayer[1](x, lambda x: self.self_attn(x, m, m, source_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    # Decoder is a collection of Decoder Layers\n",
        "    def __init__(self, layer, N):\n",
        "        '''\n",
        "        :param layer: decoder layer\n",
        "        :param N: the number of decoder layers\n",
        "        '''\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x, memory, source_mask, target_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, source_mask, target_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "        '''\n",
        "        :param d_model: embedding dimension\n",
        "        :param vocab_size: the size of the vocabulary\n",
        "        '''\n",
        "        super(Generator, self).__init__()\n",
        "        self.project = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.project(x), dim = -1)\n",
        "\n",
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder, source_embed, target_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = source_embed\n",
        "        self.tgt_embed = target_embed\n",
        "        self.generator = generator\n",
        "\n",
        "    def forward(self, source, target, source_mask, target_mask):\n",
        "        # encoded source used as memory in decode function\n",
        "        return self.decode(self.encode(source, source_mask), source_mask,\n",
        "                           target, target_mask)\n",
        "\n",
        "    def encode(self, source, source_mask):\n",
        "        return self.encoder(self.src_embed(source), source_mask)\n",
        "\n",
        "    def decode(self, memory, source_mask, target, target_mask):\n",
        "        # embedded target as x in the decoder function\n",
        "        return self.decoder(self.tgt_embed(target), memory, source_mask, target_mask)\n",
        "\n",
        "def make_model(source_vocab, target_vocab, N=6,\\\n",
        "               d_model=512, d_ff=2048, head=8, dropout=0.1):\n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(head, d_model)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn),c(ff), dropout), N),\n",
        "        # note the order of vocab_size and d_model;\n",
        "        # nn.Embedding is not the same with Embeddings;\n",
        "        # check Embedding_Encoder.py for more;\n",
        "        nn.Sequential(Embeddings(d_model, source_vocab), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, target_vocab), c(position)),\n",
        "        Generator(d_model, target_vocab)\n",
        "    )\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1: nn.init.xavier_uniform_(p) # initialization: make p uniformly sampled; check xavier_uniform for details\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Task: Test the language model's capability to copy a sequence of numbers\n",
        "      Input a list of numbers, the output should be the identical list of numbers\n",
        "SubTask: Write a data generator to generate sample data for model testing\n",
        "'''\n",
        "\n",
        "# use a function to generate data\n",
        "def data_generator(V, batch_size, num_batch):\n",
        "    '''\n",
        "    :param V: the maximal data value + 1\n",
        "    :param batch_size: sample data size of one round of training after which model parameters are updated\n",
        "    :param num_batch: number of rounds of training\n",
        "    '''\n",
        "    for i in range(num_batch):\n",
        "        # data value from 1 to V-1, with data matrix shape = batch_size times 10\n",
        "        data = torch.from_numpy(np.random.randint(1, V, size = (batch_size, 10)))\n",
        "\n",
        "        # set starting position label\n",
        "        data[:, 0] = 1\n",
        "\n",
        "        # for a copy task source data and target data should be the same\n",
        "        # no gradient calculation involved\n",
        "        source = Variable(data, requires_grad = False)\n",
        "        target = Variable(data, requires_grad = False)\n",
        "        yield Batch(source, target)\n",
        "\n",
        "\n",
        "def run(model, loss, epochs=10):\n",
        "    '''\n",
        "    :param model: model\n",
        "    :param loss: loss function\n",
        "    :param epochs: number of rounds of training\n",
        "    '''\n",
        "    for epoch in range(epochs):\n",
        "        # train model, update model parameters\n",
        "        model.train()\n",
        "        run_epoch(data_generator(V, 20, 8), model, loss)\n",
        "        # evaluate model, no parameters update\n",
        "        model.eval()\n",
        "        run_epoch(data_generator(V, 5, 8), model, loss)\n"
      ],
      "metadata": {
        "id": "q5g__pE7CWic"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate an example to demonstrate the training of the model. Commented contents are code to check the functionalities of components."
      ],
      "metadata": {
        "id": "KEdHXxGxHELW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate variables\n",
        "V = 11\n",
        "batch_size = 20\n",
        "num_batch = 30\n",
        "\n",
        "# get model\n",
        "model = make_model(V, V, N = 2)\n",
        "\n",
        "# get optimizer\n",
        "model_optimizer = get_std_opt(model)\n",
        "\n",
        "# get smooth criterion\n",
        "criterion = LabelSmoothing(size = V, padding_idx = 0, smoothing = 0.0)\n",
        "\n",
        "# get loss function\n",
        "# model as an EncoderDecoder whose last element is a generator object\n",
        "loss = SimpleLossCompute(model.generator, criterion, model_optimizer)\n",
        "if __name__ == '__main__':\n",
        "    run(model, loss)\n",
        "\n",
        "'''\n",
        "# demonstration of xavier_uniform\n",
        "w = torch.empty(3, 5)\n",
        "w = nn.init.xavier_uniform_(w, gain = nn.init.calculate_gain('relu'))\n",
        "print(w)\n",
        "\n",
        "# input parameters\n",
        "source_vocab = 11\n",
        "target_vocab = 11\n",
        "N = 6\n",
        "\n",
        "# application\n",
        "if __name__ == '__main__':\n",
        "    res = make_model(source_vocab, target_vocab, N)\n",
        "    print(res)\n",
        "    print(res.generator)\n",
        "'''\n",
        "\n",
        "'''\n",
        "d_model = 512\n",
        "vocab = 1000\n",
        "dropout = 0.1\n",
        "max_len = 60\n",
        "\n",
        "original_text = Variable(torch.LongTensor([[132,8,521,308],[491,398,999,223]]))\n",
        "emb = Embeddings(d_model, vocab)\n",
        "embr = emb(original_text)\n",
        "#print(f\"The resulting tensor after Embeddings:\\n {embr}\")\n",
        "\n",
        "# continue with embr\n",
        "x= embr\n",
        "pe = PositionalEncoding(d_model, dropout, max_len)\n",
        "pe_result = pe(x)\n",
        "#print(f\"The resulting tensor after positional encoding:\\n {pe_result}\")\n",
        "#print(f\"The shape of pe_result: {pe_result.shape}\")\n",
        "\n",
        "# continue with pe_result\n",
        "size = 512\n",
        "head = 8\n",
        "d_model = 512\n",
        "d_ff = 64\n",
        "dropout = 0.2\n",
        "c = copy.deepcopy\n",
        "attn = MultiHeadedAttention(head, d_model)\n",
        "ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "layer = EncoderLayer(size, c(attn), c(ff), dropout)\n",
        "N = 8\n",
        "mask = Variable(torch.zeros(2, 4, 4))\n",
        "\n",
        "# continue with en_result\n",
        "en = Encoder(layer, N)\n",
        "en_result = en(x, mask)\n",
        "layer = DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout)\n",
        "N = 8\n",
        "memory = en_result\n",
        "# In practice source_mask and target_mask are not the same\n",
        "mask = Variable(torch.zeros(2, 4, 4))\n",
        "source_mask = target_mask = mask\n",
        "\n",
        "# get decoder result\n",
        "de = Decoder(layer, N)\n",
        "de_result = de(x, memory, source_mask, target_mask)\n",
        "#print(de_result)\n",
        "#print(de_result.shape)\n",
        "\n",
        "\n",
        "# application\n",
        "vocab_size = 1000\n",
        "d_model= 512\n",
        "gen = Generator(d_model, vocab_size)\n",
        "source_embed = nn.Embedding(vocab_size, d_model)\n",
        "target_embed = nn.Embedding(vocab_size, d_model)\n",
        "# input parameters (assuming source and target are the same)\n",
        "source = target = Variable(torch.LongTensor([[100, 2, 421, 500], [491, 998, 1, 221]]))\n",
        "source_mask = target_mask = Variable(torch.zeros(2, 4, 4))\n",
        "\n",
        "ed = EncoderDecoder(en, de, source_embed, target_embed, gen)\n",
        "ed_result = ed(source, target, source_mask, target_mask)\n",
        "print(ed_result)\n",
        "print(ed_result.shape)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "M_FcF3EqEmy8",
        "outputId": "85603ad7-e0c4-4de1-b2a8-008441729c16"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch Step: 1 Loss: 3.279118 Tokens per Sec: 372.940277\n",
            "Epoch Step: 1 Loss: 3.188324 Tokens per Sec: 238.977463\n",
            "Epoch Step: 1 Loss: 3.076892 Tokens per Sec: 419.207275\n",
            "Epoch Step: 1 Loss: 2.559503 Tokens per Sec: 166.714081\n",
            "Epoch Step: 1 Loss: 2.650356 Tokens per Sec: 418.882477\n",
            "Epoch Step: 1 Loss: 2.144870 Tokens per Sec: 247.651245\n",
            "Epoch Step: 1 Loss: 2.441571 Tokens per Sec: 443.532562\n",
            "Epoch Step: 1 Loss: 2.110667 Tokens per Sec: 249.359802\n",
            "Epoch Step: 1 Loss: 2.138038 Tokens per Sec: 292.783783\n",
            "Epoch Step: 1 Loss: 1.910373 Tokens per Sec: 247.901703\n",
            "Epoch Step: 1 Loss: 2.024286 Tokens per Sec: 419.112701\n",
            "Epoch Step: 1 Loss: 1.892929 Tokens per Sec: 242.006073\n",
            "Epoch Step: 1 Loss: 2.044051 Tokens per Sec: 435.513428\n",
            "Epoch Step: 1 Loss: 1.915974 Tokens per Sec: 187.736664\n",
            "Epoch Step: 1 Loss: 1.988847 Tokens per Sec: 442.701874\n",
            "Epoch Step: 1 Loss: 1.682120 Tokens per Sec: 253.315094\n",
            "Epoch Step: 1 Loss: 1.928860 Tokens per Sec: 346.934662\n",
            "Epoch Step: 1 Loss: 1.551572 Tokens per Sec: 192.432816\n",
            "Epoch Step: 1 Loss: 1.819800 Tokens per Sec: 346.490662\n",
            "Epoch Step: 1 Loss: 1.700040 Tokens per Sec: 256.939362\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nd_model = 512\\nvocab = 1000\\ndropout = 0.1\\nmax_len = 60\\n\\noriginal_text = Variable(torch.LongTensor([[132,8,521,308],[491,398,999,223]]))\\nemb = Embeddings(d_model, vocab)\\nembr = emb(original_text)\\n#print(f\"The resulting tensor after Embeddings:\\n {embr}\")\\n\\n# continue with embr\\nx= embr\\npe = PositionalEncoding(d_model, dropout, max_len)\\npe_result = pe(x)\\n#print(f\"The resulting tensor after positional encoding:\\n {pe_result}\")\\n#print(f\"The shape of pe_result: {pe_result.shape}\")\\n\\n# continue with pe_result\\nsize = 512\\nhead = 8\\nd_model = 512\\nd_ff = 64\\ndropout = 0.2\\nc = copy.deepcopy\\nattn = MultiHeadedAttention(head, d_model)\\nff = PositionwiseFeedForward(d_model, d_ff, dropout)\\nlayer = EncoderLayer(size, c(attn), c(ff), dropout)\\nN = 8\\nmask = Variable(torch.zeros(2, 4, 4))\\n\\n# continue with en_result\\nen = Encoder(layer, N)\\nen_result = en(x, mask)\\nlayer = DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout)\\nN = 8\\nmemory = en_result\\n# In practice source_mask and target_mask are not the same\\nmask = Variable(torch.zeros(2, 4, 4))\\nsource_mask = target_mask = mask\\n\\n# get decoder result\\nde = Decoder(layer, N)\\nde_result = de(x, memory, source_mask, target_mask)\\n#print(de_result)\\n#print(de_result.shape)\\n\\n\\n# application\\nvocab_size = 1000\\nd_model= 512\\ngen = Generator(d_model, vocab_size)\\nsource_embed = nn.Embedding(vocab_size, d_model)\\ntarget_embed = nn.Embedding(vocab_size, d_model)\\n# input parameters (assuming source and target are the same)\\nsource = target = Variable(torch.LongTensor([[100, 2, 421, 500], [491, 998, 1, 221]]))\\nsource_mask = target_mask = Variable(torch.zeros(2, 4, 4))\\n\\ned = EncoderDecoder(en, de, source_embed, target_embed, gen)\\ned_result = ed(source, target, source_mask, target_mask)\\nprint(ed_result)\\nprint(ed_result.shape)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}