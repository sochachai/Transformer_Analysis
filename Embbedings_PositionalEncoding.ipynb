{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMWnAbJp3E3xdThVJDjxas6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sochachai/Transformer_Analysis/blob/main/Embbedings_PositionalEncoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load packages"
      ],
      "metadata": {
        "id": "QadEcdM9HWsP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZL5Plt3mvusS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct the class of Embeddings and Positional Encoding."
      ],
      "metadata": {
        "id": "UBTCCLP4Hc3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        '''\n",
        "        :param d_model: embedding dimension\n",
        "        :param vocab: size of vocabulary\n",
        "        '''\n",
        "        # Initialization\n",
        "        super(Embeddings, self).__init__()\n",
        "        # Defrine a word embedding object\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        # Instantiate d_model\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        :param x: tensor representing the original text\n",
        "        '''\n",
        "        return self.lut(x) * math.sqrt(self.d_model)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout, max_len = 5000):\n",
        "        '''\n",
        "        :param d_model: dimension of the encoding\n",
        "        :param dropout: dropout rate from 0 to 1\n",
        "        :param max_len: the maximum length of a sentence\n",
        "        '''\n",
        "        # Inherit the initialization of nn.Module\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # Objectify dropout\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Inherit a positional encoder matrix, max_len * d_model\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        # Inherit an absolute position matrix, max_len * 1\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "\n",
        "        # Define the conversion matrix, initialization with gap = 2\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0)/d_model))\n",
        "\n",
        "        # Copy the absolute position matrix to the positional encoder matrix\n",
        "        # by sine and cosine wave according to the parity of column indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) # even indiced columns are imputed by sine\n",
        "        pe[:, 1::2] = torch.cos(position * div_term) # odd indiced columns are imputed by cosine\n",
        "\n",
        "        # Extend pe to 3-dimensional tensor\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # Register pe to a buffer, the buffer is not a parameter of the class\n",
        "        # the buffer will not be updated along with the model update\n",
        "        # but it can be loaded along with the model\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        :param x: Tensor of text\n",
        "        :return: x + the positional encoding\n",
        "        '''\n",
        "        # Shrink the size of pe to save storage\n",
        "        # by converting the second dimension, i.e. the dimension of max_len\n",
        "        # to the size of the sentence len of x, i.e. the second dimension of x\n",
        "        x = x + Variable(self.pe[:,:x.size(1)], requires_grad = False) # False: pe will not be updated\n",
        "        return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "q5g__pE7CWic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate an example to demonstrate the use of Embeddings and Positional Encoding."
      ],
      "metadata": {
        "id": "KEdHXxGxHELW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "vocab = 1000\n",
        "dropout = 0.1\n",
        "max_len = 60\n",
        "\n",
        "original_text = Variable(torch.LongTensor([[132,8,521,308],[491,398,999,223]]))\n",
        "emb = Embeddings(d_model, vocab)\n",
        "embr = emb(original_text)\n",
        "print(f\"The resulting tensor after Embeddings:\\n {embr}\")\n",
        "\n",
        "x= embr\n",
        "pe = PositionalEncoding(d_model, dropout, max_len)\n",
        "pe_result = pe(x)\n",
        "print(f\"The resulting tensor after positional encoding:\\n {pe_result}\")\n",
        "print(f\"The shape of pe_result: {pe_result.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_FcF3EqEmy8",
        "outputId": "d2e65dc1-afb7-496a-91e8-129e337d39e7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The resulting tensor after Embeddings:\n",
            " tensor([[[ -1.6712, -12.1986,  -7.6789,  ...,  13.3293, -16.7797, -23.3994],\n",
            "         [ 30.2555, -10.3327,  14.4755,  ...,   4.6873, -26.1814,  17.1574],\n",
            "         [ 26.8043,  12.9459,  14.8024,  ...,  30.4907, -27.0993,  20.5009],\n",
            "         [-14.7976,  20.7444, -53.3502,  ...,  19.1950,  40.0398,  17.6165]],\n",
            "\n",
            "        [[-36.4085,   8.8077,  17.4784,  ...,  13.9540,  33.7637, -23.9044],\n",
            "         [ 32.3228,  46.4586, -23.2815,  ..., -12.5101,  -7.3369, -11.7269],\n",
            "         [ 25.0436,   3.4608, -44.5584,  ...,   6.7308,  -7.6718,   3.3365],\n",
            "         [ 12.8997,   4.5520,  13.7686,  ...,  -0.1834, -41.6246,   7.7329]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "The resulting tensor after positional encoding:\n",
            " tensor([[[ -1.8568, -12.4429,  -8.5321,  ...,  15.9214, -18.6441,  -0.0000],\n",
            "         [ 34.5522,  -0.0000,  16.9971,  ...,   6.3193, -29.0903,  20.1748],\n",
            "         [  0.0000,  13.9220,  17.4876,  ...,  34.9897, -30.1101,  23.8899],\n",
            "         [-16.2850,   0.0000, -59.0056,  ...,  22.4388,  44.4890,  20.6850]],\n",
            "\n",
            "        [[-40.4539,  10.8975,  19.4204,  ...,  16.6155,  37.5153, -25.4493],\n",
            "         [ 36.8492,  52.2210, -24.9552,  ...,  -0.0000,  -8.1520, -11.9188],\n",
            "         [ 28.8365,   0.0000, -48.4689,  ...,   8.5898,  -8.5240,   4.8183],\n",
            "         [ 14.4898,   3.9578,  15.5707,  ...,   0.9073,  -0.0000,   0.0000]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "The shape of pe_result: torch.Size([2, 4, 512])\n"
          ]
        }
      ]
    }
  ]
}