{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCEJ93IXe+UukehHGNeWZO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sochachai/Transformer_Analysis/blob/main/10_digits_model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load packages"
      ],
      "metadata": {
        "id": "QadEcdM9HWsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check directory\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8gwuRviI0JD",
        "outputId": "529e3f89-d078-4cf2-a74c-1a77ea410263"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# confirm the assistant py file has been uploaded to the correct directory\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kj093MmNI24A",
        "outputId": "2226b813-7b15-41c5-9d88-ef9e6e46362d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my_transformer_utils.py  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check Python version\n",
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9lyQC3sJbJs",
        "outputId": "0cab66ba-3950-4dad-d4d3-00f05fb224c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install torch\n",
        "!pip install torch==2.2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRb0L4jOJzuY",
        "outputId": "17b86ad1-09ca-435b-9d4f-13db3606b4f6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.2.0\n",
            "  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.2.0 (from torch==2.2.0)\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.0) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 torch-2.2.0 triton-2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check torch version\n",
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AWYB4ZIJnlB",
        "outputId": "711a9f8f-48ad-4fff-9988-5a1d9a44b9e3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZL5Plt3mvusS"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "# Issue caused by torch version described below:\n",
        "# the original package should be pyitcast.transformer_utils\n",
        "# the problem is that pyitcast.transformer_utils relies on an old version of torch (1.3.1 should work)\n",
        "# and causes the error \"IndexError: invalid index of a 0-dim tensor. Use `tensor.item()` in Python...\"\n",
        "# but the installation of an old version of torch that match pyitcast.transformer is not trivial\n",
        "# versions of 1.11.0 or after is not compatible with pyitcast.transformer and installation of them will not\n",
        "# solve the issue\n",
        "# To solve this issue:\n",
        "# download the pyitcast.transformer_utils (open by clicking the error message) as a py file\n",
        "# modify the last line of the SimpleLossCompute class from \"return loss.data[0] * norm\"\n",
        "# to \"return loss.data * norm\"\n",
        "\n",
        "from my_transformer_utils import Batch\n",
        "from my_transformer_utils import run_epoch\n",
        "from my_transformer_utils import greedy_decode\n",
        "from my_transformer_utils import get_std_opt # get_std_opt is based on Adam optimizer\n",
        "from my_transformer_utils import LabelSmoothing # offset human label errors to prevent overfitting\n",
        "from my_transformer_utils import SimpleLossCompute # calculate loss after smoothing, use cross_entropy_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct the class of Embeddings and Positional Encoding."
      ],
      "metadata": {
        "id": "UBTCCLP4Hc3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        '''\n",
        "        :param d_model: embedding dimension\n",
        "        :param vocab: size of vocabulary\n",
        "        '''\n",
        "        # Initialization\n",
        "        super(Embeddings, self).__init__()\n",
        "        # Defrine a word embedding object\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        # Instantiate d_model\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        :param x: tensor representing the original text\n",
        "        '''\n",
        "        return self.lut(x) * math.sqrt(self.d_model)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout, max_len = 5000):\n",
        "        '''\n",
        "        :param d_model: dimension of the encoding\n",
        "        :param dropout: dropout rate from 0 to 1\n",
        "        :param max_len: the maximum length of a sentence\n",
        "        '''\n",
        "        # Inherit the initialization of nn.Module\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # Objectify dropout\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Inherit a positional encoder matrix, max_len * d_model\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        # Inherit an absolute position matrix, max_len * 1\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "\n",
        "        # Define the conversion matrix, initialization with gap = 2\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0)/d_model))\n",
        "\n",
        "        # Copy the absolute position matrix to the positional encoder matrix\n",
        "        # by sine and cosine wave according to the parity of column indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) # even indiced columns are imputed by sine\n",
        "        pe[:, 1::2] = torch.cos(position * div_term) # odd indiced columns are imputed by cosine\n",
        "\n",
        "        # Extend pe to 3-dimensional tensor\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # Register pe to a buffer, the buffer is not a parameter of the class\n",
        "        # the buffer will not be updated along with the model update\n",
        "        # but it can be loaded along with the model\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        :param x: Tensor of text\n",
        "        :return: x + the positional encoding\n",
        "        '''\n",
        "        # Shrink the size of pe to save storage\n",
        "        # by converting the second dimension, i.e. the dimension of max_len\n",
        "        # to the size of the sentence len of x, i.e. the second dimension of x\n",
        "        x = x + Variable(self.pe[:,:x.size(1)], requires_grad = False) # False: pe will not be updated\n",
        "        return self.dropout(x)\n",
        "\n",
        "def attention(query, key, value, mask = None, dropout = None):\n",
        "    '''\n",
        "    :param query: vectorized original text\n",
        "    :param key: key words of text\n",
        "    :param value: the original value of key, summarization of query\n",
        "    :param mask: hide words to avoid data leakage\n",
        "    :param dropout: dropout rate of neural network\n",
        "    :return:\n",
        "    '''\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9) # compare each position with 0\n",
        "\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "\n",
        "    return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "def clones(module, N):\n",
        "    '''\n",
        "    :param module: one attention layer\n",
        "    :param N: the number of module\n",
        "    '''\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)]) # deepcopy uses a different memory\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, head, embedding_dim, dropout = 0.1):\n",
        "        '''\n",
        "        :param head: the number of heads\n",
        "        :param embedding_dim: the embedding dimension\n",
        "        :param dropout: default dropout rate set to 0.1\n",
        "        '''\n",
        "        # Inherit the initialization\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        # head must be an integral factor of embedding_dim\n",
        "        assert embedding_dim % head == 0\n",
        "        # each head is assigned with the following dimension\n",
        "        self.d_k = embedding_dim // head # division in the integral domain Z\n",
        "        # substantiate head\n",
        "        self.head = head\n",
        "        # create linear layers, we need 4 of them for Q, K, V and the final connection\n",
        "        self.linears = clones(nn.Linear(embedding_dim, embedding_dim), 4)\n",
        "        # create the attention and dropout rate\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p = dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # We have 4 linears and only zipping the first 3 with Q, K, V, the last linear is not used.\n",
        "        # view(batch_size, -1, self.head, self.d_k).transpose(1,2)\n",
        "        # is not the same with view(batch_size, self.head, -1, self.d_k)\n",
        "        # self.head and self.d_k should be neighboring to get embedding_dim in order for the tensor\n",
        "        # to interpret the relationship between the meaning of words of their positions in a sentence\n",
        "\n",
        "        query, key, value = \\\n",
        "            [model(x).view(batch_size, -1, self.head, self.d_k).transpose(1,2)\n",
        "             for model, x in zip(self.linears, (query, key, value))]\n",
        "\n",
        "        x, self.attn = attention(query, key, value, mask = mask, dropout = self.dropout)\n",
        "\n",
        "        # Reshape x\n",
        "        # must use contiguous method after the transpose before the view method\n",
        "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.head * self.d_k)\n",
        "\n",
        "        # Pass x to the 4th linear layer\n",
        "        return self.linears[-1](x)\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        '''\n",
        "        :param d_model: embedding dimension\n",
        "        :param d_ff: transitional dimension\n",
        "        :param dropout: default dropout set to 0.1\n",
        "        '''\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w1 = nn.Linear(d_model, d_ff)\n",
        "        self.w2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w2(self.dropout(F.relu(self.w1(x))))\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, features, eps = 1e-6):\n",
        "        '''\n",
        "        :param features: embedding dimension\n",
        "        :param eps: avoid zero denominator\n",
        "        '''\n",
        "        super(LayerNorm, self).__init__()\n",
        "        # nn.Parameter formats the variables to intrinsic parameters\n",
        "        # they will be updated along with the model in contrast with buffer\n",
        "        self.a2 = nn.Parameter(torch.ones(features))\n",
        "        self.b2 = nn.Parameter(torch.zeros(features))\n",
        "        # initialization of eps\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        :param x: the output of previous layer\n",
        "        :return: numerical standardized x\n",
        "        '''\n",
        "        mean = x.mean(-1, keepdim = True)\n",
        "        std = x.std(-1, keepdim = True)\n",
        "        return self.a2 * (x - mean) / (std + self.eps) + self.b2\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "    def __init__(self, size, dropout = 0.1):\n",
        "        '''\n",
        "        :param size: embedding size\n",
        "        :param dropout: deactivation of neurons to avoid overfitting\n",
        "        '''\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        '''\n",
        "        :param x: the output of previous layer\n",
        "        :param sublayer: a function, e.g. Multihead_Attention, PositionwiseFeedForward etc.\n",
        "        :return: x plus sublayer functioning on normed x with dropout\n",
        "        '''\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        '''\n",
        "        :param size: embedding dimension\n",
        "        :param self_attn: attention\n",
        "        :param feed_forward: positionwise feed forward\n",
        "        :param dropout: avoid overfitting\n",
        "        '''\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        '''\n",
        "        :param x: output of previous layer\n",
        "        :param mask: tensor mask to prevent data leakage\n",
        "        '''\n",
        "        # input matrix followed by operating function, returns an output matrix\n",
        "        # sublayer(x, function) will return function(x)\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) # the forward function of multihead attention\n",
        "        return self.sublayer[1](x, self.feed_forward) # the forward function of pointwise feedforward\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    # Encoder is a collection of Encoder Layers\n",
        "    def __init__(self, layer, N):\n",
        "        '''\n",
        "        :param layer: one encoder layer\n",
        "        :param N: the number of encoder layers\n",
        "        '''\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers: x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        '''\n",
        "        :param size: embedding dimension\n",
        "        :param self_attn: masked multihead attention\n",
        "        :param src_attn: multihead attention\n",
        "        :param feed_forward: pointwise feed forward\n",
        "        :param dropout: avoid overfitting\n",
        "        '''\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        # 3 sublayer for a decoder layer\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        "\n",
        "    def forward(self, x, memory, source_mask, target_mask):\n",
        "        '''\n",
        "        :param x: output of previous layer\n",
        "        :param memory: result of encoder\n",
        "        :param source_mask: delete unnecessary info to improve model performance\n",
        "        :param target_mask: hide info to prevent data leakage\n",
        "        :return: output tensor\n",
        "        '''\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, target_mask))\n",
        "        # for second layer, Q = x, K = V = m\n",
        "        x = self.sublayer[1](x, lambda x: self.self_attn(x, m, m, source_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    # Decoder is a collection of Decoder Layers\n",
        "    def __init__(self, layer, N):\n",
        "        '''\n",
        "        :param layer: decoder layer\n",
        "        :param N: the number of decoder layers\n",
        "        '''\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x, memory, source_mask, target_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, source_mask, target_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "        '''\n",
        "        :param d_model: embedding dimension\n",
        "        :param vocab_size: the size of the vocabulary\n",
        "        '''\n",
        "        super(Generator, self).__init__()\n",
        "        self.project = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.project(x), dim = -1)\n",
        "\n",
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder, source_embed, target_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = source_embed\n",
        "        self.tgt_embed = target_embed\n",
        "        self.generator = generator\n",
        "\n",
        "    def forward(self, source, target, source_mask, target_mask):\n",
        "        # encoded source used as memory in decode function\n",
        "        return self.decode(self.encode(source, source_mask), source_mask,\n",
        "                           target, target_mask)\n",
        "\n",
        "    def encode(self, source, source_mask):\n",
        "        return self.encoder(self.src_embed(source), source_mask)\n",
        "\n",
        "    def decode(self, memory, source_mask, target, target_mask):\n",
        "        # embedded target as x in the decoder function\n",
        "        return self.decoder(self.tgt_embed(target), memory, source_mask, target_mask)\n",
        "\n",
        "def make_model(source_vocab, target_vocab, N=6,\\\n",
        "               d_model=512, d_ff=2048, head=8, dropout=0.1):\n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(head, d_model)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn),c(ff), dropout), N),\n",
        "        # note the order of vocab_size and d_model;\n",
        "        # nn.Embedding is not the same with Embeddings;\n",
        "        # check Embedding_Encoder.py for more;\n",
        "        nn.Sequential(Embeddings(d_model, source_vocab), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, target_vocab), c(position)),\n",
        "        Generator(d_model, target_vocab)\n",
        "    )\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1: nn.init.xavier_uniform_(p) # initialization: make p uniformly sampled; check xavier_uniform for details\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Task: Test the language model's capability to copy a sequence of numbers\n",
        "      Input a list of numbers, the output should be the identical list of numbers\n",
        "SubTask: Write a data generator to generate sample data for model testing\n",
        "'''\n",
        "\n",
        "# use a function to generate data\n",
        "def data_generator(V, batch_size, num_batch):\n",
        "    '''\n",
        "    :param V: the maximal data value + 1\n",
        "    :param batch_size: sample data size of one round of training after which model parameters are updated\n",
        "    :param num_batch: number of rounds of training\n",
        "    '''\n",
        "    for i in range(num_batch):\n",
        "        # data value from 1 to V-1, with data matrix shape = batch_size times 10\n",
        "        data = torch.from_numpy(np.random.randint(1, V, size = (batch_size, 10)))\n",
        "\n",
        "        # set starting position label\n",
        "        data[:, 0] = 1\n",
        "\n",
        "        # for a copy task source data and target data should be the same\n",
        "        # no gradient calculation involved\n",
        "        source = Variable(data, requires_grad = False)\n",
        "        target = Variable(data, requires_grad = False)\n",
        "        yield Batch(source, target)\n",
        "\n",
        "\n",
        "def run(model, loss, epochs=10):\n",
        "    '''\n",
        "    :param model: model\n",
        "    :param loss: loss function\n",
        "    :param epochs: number of rounds of training\n",
        "    '''\n",
        "    for epoch in range(epochs):\n",
        "        # train model, update model parameters\n",
        "        model.train()\n",
        "        run_epoch(data_generator(V, 200, 8), model, loss)\n",
        "        # evaluate model, no parameters update\n",
        "        model.eval()\n",
        "        run_epoch(data_generator(V, 50, 8), model, loss)\n"
      ],
      "metadata": {
        "id": "q5g__pE7CWic"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate a model."
      ],
      "metadata": {
        "id": "KEdHXxGxHELW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate variables\n",
        "V = 11\n",
        "batch_size = 20\n",
        "num_batch = 30\n",
        "\n",
        "# get model\n",
        "model = make_model(V, V, N = 2)\n",
        "\n",
        "# get optimizer\n",
        "model_optimizer = get_std_opt(model)\n",
        "\n",
        "# get smooth criterion\n",
        "criterion = LabelSmoothing(size = V, padding_idx = 0, smoothing = 0.0)\n",
        "\n",
        "# get loss function\n",
        "# model as an EncoderDecoder whose last element is a generator object\n",
        "loss = SimpleLossCompute(model.generator, criterion, model_optimizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_FcF3EqEmy8",
        "outputId": "affe44c4-4bc6-42b6-a55d-e56e228e2232"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10 epochs of model training."
      ],
      "metadata": {
        "id": "Gf1eGbnNFeB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model training\n",
        "epochs = 10\n",
        "if __name__ == '__main__':\n",
        "    run(model, loss, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLdoV2-vsitL",
        "outputId": "e80c5a6b-007a-4698-9547-c0533a37fd10"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch Step: 1 Loss: 2.961393 Tokens per Sec: 424.990875\n",
            "Epoch Step: 1 Loss: 2.667557 Tokens per Sec: 524.896240\n",
            "Epoch Step: 1 Loss: 2.682182 Tokens per Sec: 484.157135\n",
            "Epoch Step: 1 Loss: 2.374245 Tokens per Sec: 361.784180\n",
            "Epoch Step: 1 Loss: 2.348002 Tokens per Sec: 480.880524\n",
            "Epoch Step: 1 Loss: 2.027013 Tokens per Sec: 495.559052\n",
            "Epoch Step: 1 Loss: 2.147898 Tokens per Sec: 416.975433\n",
            "Epoch Step: 1 Loss: 1.852099 Tokens per Sec: 389.495911\n",
            "Epoch Step: 1 Loss: 1.956431 Tokens per Sec: 485.372101\n",
            "Epoch Step: 1 Loss: 1.748399 Tokens per Sec: 484.480286\n",
            "Epoch Step: 1 Loss: 1.836280 Tokens per Sec: 522.639221\n",
            "Epoch Step: 1 Loss: 1.547739 Tokens per Sec: 269.327820\n",
            "Epoch Step: 1 Loss: 1.757527 Tokens per Sec: 509.225281\n",
            "Epoch Step: 1 Loss: 1.486226 Tokens per Sec: 509.723511\n",
            "Epoch Step: 1 Loss: 1.685471 Tokens per Sec: 553.690430\n",
            "Epoch Step: 1 Loss: 1.337506 Tokens per Sec: 503.662384\n",
            "Epoch Step: 1 Loss: 1.541792 Tokens per Sec: 552.025024\n",
            "Epoch Step: 1 Loss: 1.130561 Tokens per Sec: 524.129761\n",
            "Epoch Step: 1 Loss: 1.409600 Tokens per Sec: 511.749176\n",
            "Epoch Step: 1 Loss: 0.900416 Tokens per Sec: 521.524475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model's capibility of copying the original sequence with greedy decode after 10 epochs of training."
      ],
      "metadata": {
        "id": "HpkNeAqqr4Yw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(an_integer_sequence_of_size_10):\n",
        "    # greedy decode\n",
        "    # enter evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # get source input\n",
        "    source = Variable(torch.LongTensor([an_integer_sequence_of_size_10]))\n",
        "\n",
        "    # get source mask\n",
        "    # 1 for no masking\n",
        "    source_mask = Variable(torch.ones(1, 1, 10))\n",
        "\n",
        "    # get result\n",
        "    result = greedy_decode(model, source, source_mask, max_len=10, start_symbol=1)\n",
        "    print(f\"The source numeric sequence is:\\n {source}\")\n",
        "    print(f\"The resulting numeric sequence after {epochs} epochs of training is:\\n {result}\")\n",
        "\n",
        "test_model([1,  3,  2,  5,  4,  6,  7,  8,  9, 10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cfZ2TBzryq5",
        "outputId": "a1a9b92d-4ef9-4c44-813a-14d411cb5a7b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The source numeric sequence is:\n",
            " tensor([[ 1,  3,  2,  5,  4,  6,  7,  8,  9, 10]])\n",
            "The resulting numeric sequence after 10 epochs of training is:\n",
            " tensor([[ 1,  3,  2,  5,  4,  6,  7,  8, 10,  9]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model's capibility of copying the original sequence with greedy decode after an extra of 100 epochs of training."
      ],
      "metadata": {
        "id": "tEXOpUlSGGiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model training\n",
        "epochs = 100\n",
        "if __name__ == '__main__':\n",
        "    run(model, loss, epochs)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5SwQh1vGDRJ",
        "outputId": "742a2698-cf4a-447b-b6b5-dac208bfb0b5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch Step: 1 Loss: 1.126421 Tokens per Sec: 470.126404\n",
            "Epoch Step: 1 Loss: 0.630473 Tokens per Sec: 381.614624\n",
            "Epoch Step: 1 Loss: 0.981566 Tokens per Sec: 496.690491\n",
            "Epoch Step: 1 Loss: 0.398861 Tokens per Sec: 523.217957\n",
            "Epoch Step: 1 Loss: 0.812244 Tokens per Sec: 538.142273\n",
            "Epoch Step: 1 Loss: 0.260811 Tokens per Sec: 517.303833\n",
            "Epoch Step: 1 Loss: 0.637240 Tokens per Sec: 521.616089\n",
            "Epoch Step: 1 Loss: 0.151112 Tokens per Sec: 515.750671\n",
            "Epoch Step: 1 Loss: 0.526027 Tokens per Sec: 482.231232\n",
            "Epoch Step: 1 Loss: 0.080227 Tokens per Sec: 362.150269\n",
            "Epoch Step: 1 Loss: 0.405073 Tokens per Sec: 477.409027\n",
            "Epoch Step: 1 Loss: 0.063735 Tokens per Sec: 481.482605\n",
            "Epoch Step: 1 Loss: 0.339040 Tokens per Sec: 509.230072\n",
            "Epoch Step: 1 Loss: 0.032460 Tokens per Sec: 509.434113\n",
            "Epoch Step: 1 Loss: 0.272131 Tokens per Sec: 565.656982\n",
            "Epoch Step: 1 Loss: 0.035680 Tokens per Sec: 539.612671\n",
            "Epoch Step: 1 Loss: 0.266207 Tokens per Sec: 520.860474\n",
            "Epoch Step: 1 Loss: 0.012947 Tokens per Sec: 504.193695\n",
            "Epoch Step: 1 Loss: 0.204639 Tokens per Sec: 485.656219\n",
            "Epoch Step: 1 Loss: 0.008554 Tokens per Sec: 497.923767\n",
            "Epoch Step: 1 Loss: 0.196960 Tokens per Sec: 469.834930\n",
            "Epoch Step: 1 Loss: 0.014060 Tokens per Sec: 489.634399\n",
            "Epoch Step: 1 Loss: 0.165041 Tokens per Sec: 497.873383\n",
            "Epoch Step: 1 Loss: 0.008504 Tokens per Sec: 351.409576\n",
            "Epoch Step: 1 Loss: 0.138550 Tokens per Sec: 473.790680\n",
            "Epoch Step: 1 Loss: 0.021245 Tokens per Sec: 540.367615\n",
            "Epoch Step: 1 Loss: 0.173572 Tokens per Sec: 549.449768\n",
            "Epoch Step: 1 Loss: 0.004738 Tokens per Sec: 523.140747\n",
            "Epoch Step: 1 Loss: 0.166404 Tokens per Sec: 568.631348\n",
            "Epoch Step: 1 Loss: 0.008956 Tokens per Sec: 542.047607\n",
            "Epoch Step: 1 Loss: 0.127057 Tokens per Sec: 482.118011\n",
            "Epoch Step: 1 Loss: 0.017058 Tokens per Sec: 522.349792\n",
            "Epoch Step: 1 Loss: 0.103462 Tokens per Sec: 498.126343\n",
            "Epoch Step: 1 Loss: 0.001713 Tokens per Sec: 352.956573\n",
            "Epoch Step: 1 Loss: 0.117678 Tokens per Sec: 484.264282\n",
            "Epoch Step: 1 Loss: 0.000532 Tokens per Sec: 543.174561\n",
            "Epoch Step: 1 Loss: 0.077806 Tokens per Sec: 548.817383\n",
            "Epoch Step: 1 Loss: 0.002415 Tokens per Sec: 536.022949\n",
            "Epoch Step: 1 Loss: 0.104341 Tokens per Sec: 538.785339\n",
            "Epoch Step: 1 Loss: 0.014373 Tokens per Sec: 524.698853\n",
            "Epoch Step: 1 Loss: 0.082392 Tokens per Sec: 484.372650\n",
            "Epoch Step: 1 Loss: 0.001784 Tokens per Sec: 514.415283\n",
            "Epoch Step: 1 Loss: 0.092680 Tokens per Sec: 499.818146\n",
            "Epoch Step: 1 Loss: 0.007187 Tokens per Sec: 338.050690\n",
            "Epoch Step: 1 Loss: 0.090514 Tokens per Sec: 366.284973\n",
            "Epoch Step: 1 Loss: 0.023878 Tokens per Sec: 490.032562\n",
            "Epoch Step: 1 Loss: 0.064397 Tokens per Sec: 553.798462\n",
            "Epoch Step: 1 Loss: 0.005010 Tokens per Sec: 509.024200\n",
            "Epoch Step: 1 Loss: 0.073881 Tokens per Sec: 542.494751\n",
            "Epoch Step: 1 Loss: 0.004034 Tokens per Sec: 520.482361\n",
            "Epoch Step: 1 Loss: 0.062078 Tokens per Sec: 541.910156\n",
            "Epoch Step: 1 Loss: 0.019024 Tokens per Sec: 498.009674\n",
            "Epoch Step: 1 Loss: 0.083483 Tokens per Sec: 495.817810\n",
            "Epoch Step: 1 Loss: 0.009603 Tokens per Sec: 381.748230\n",
            "Epoch Step: 1 Loss: 0.075015 Tokens per Sec: 495.251678\n",
            "Epoch Step: 1 Loss: 0.003743 Tokens per Sec: 538.486145\n",
            "Epoch Step: 1 Loss: 0.112088 Tokens per Sec: 540.370544\n",
            "Epoch Step: 1 Loss: 0.022560 Tokens per Sec: 526.891479\n",
            "Epoch Step: 1 Loss: 0.055297 Tokens per Sec: 548.266357\n",
            "Epoch Step: 1 Loss: 0.001812 Tokens per Sec: 529.912231\n",
            "Epoch Step: 1 Loss: 0.088700 Tokens per Sec: 492.145538\n",
            "Epoch Step: 1 Loss: 0.001299 Tokens per Sec: 533.345642\n",
            "Epoch Step: 1 Loss: 0.064402 Tokens per Sec: 483.664001\n",
            "Epoch Step: 1 Loss: 0.004921 Tokens per Sec: 433.851929\n",
            "Epoch Step: 1 Loss: 0.079796 Tokens per Sec: 534.747803\n",
            "Epoch Step: 1 Loss: 0.031751 Tokens per Sec: 548.293579\n",
            "Epoch Step: 1 Loss: 0.125145 Tokens per Sec: 555.417969\n",
            "Epoch Step: 1 Loss: 0.008996 Tokens per Sec: 523.479492\n",
            "Epoch Step: 1 Loss: 0.048771 Tokens per Sec: 522.274902\n",
            "Epoch Step: 1 Loss: 0.009606 Tokens per Sec: 515.770691\n",
            "Epoch Step: 1 Loss: 0.050968 Tokens per Sec: 480.303162\n",
            "Epoch Step: 1 Loss: 0.002125 Tokens per Sec: 415.419067\n",
            "Epoch Step: 1 Loss: 0.049515 Tokens per Sec: 485.124451\n",
            "Epoch Step: 1 Loss: 0.000996 Tokens per Sec: 498.825439\n",
            "Epoch Step: 1 Loss: 0.051864 Tokens per Sec: 542.980713\n",
            "Epoch Step: 1 Loss: 0.007780 Tokens per Sec: 515.227905\n",
            "Epoch Step: 1 Loss: 0.053791 Tokens per Sec: 567.038208\n",
            "Epoch Step: 1 Loss: 0.007845 Tokens per Sec: 504.539490\n",
            "Epoch Step: 1 Loss: 0.045953 Tokens per Sec: 506.910126\n",
            "Epoch Step: 1 Loss: 0.053753 Tokens per Sec: 528.715576\n",
            "Epoch Step: 1 Loss: 0.065659 Tokens per Sec: 485.348419\n",
            "Epoch Step: 1 Loss: 0.001055 Tokens per Sec: 531.011108\n",
            "Epoch Step: 1 Loss: 0.060256 Tokens per Sec: 507.992645\n",
            "Epoch Step: 1 Loss: 0.002308 Tokens per Sec: 549.022583\n",
            "Epoch Step: 1 Loss: 0.049534 Tokens per Sec: 546.947754\n",
            "Epoch Step: 1 Loss: 0.047537 Tokens per Sec: 525.636841\n",
            "Epoch Step: 1 Loss: 0.091155 Tokens per Sec: 551.287476\n",
            "Epoch Step: 1 Loss: 0.002868 Tokens per Sec: 545.850525\n",
            "Epoch Step: 1 Loss: 0.043938 Tokens per Sec: 477.080597\n",
            "Epoch Step: 1 Loss: 0.002000 Tokens per Sec: 529.809326\n",
            "Epoch Step: 1 Loss: 0.060311 Tokens per Sec: 496.137512\n",
            "Epoch Step: 1 Loss: 0.004072 Tokens per Sec: 363.649872\n",
            "Epoch Step: 1 Loss: 0.082466 Tokens per Sec: 482.316742\n",
            "Epoch Step: 1 Loss: 0.029199 Tokens per Sec: 555.494751\n",
            "Epoch Step: 1 Loss: 0.062879 Tokens per Sec: 541.002502\n",
            "Epoch Step: 1 Loss: 0.000622 Tokens per Sec: 506.508972\n",
            "Epoch Step: 1 Loss: 0.037399 Tokens per Sec: 544.044922\n",
            "Epoch Step: 1 Loss: 0.014787 Tokens per Sec: 527.710144\n",
            "Epoch Step: 1 Loss: 0.159106 Tokens per Sec: 499.688171\n",
            "Epoch Step: 1 Loss: 0.043799 Tokens per Sec: 506.832001\n",
            "Epoch Step: 1 Loss: 0.040840 Tokens per Sec: 482.625000\n",
            "Epoch Step: 1 Loss: 0.001369 Tokens per Sec: 475.644196\n",
            "Epoch Step: 1 Loss: 0.038890 Tokens per Sec: 555.285828\n",
            "Epoch Step: 1 Loss: 0.000331 Tokens per Sec: 547.849548\n",
            "Epoch Step: 1 Loss: 0.029920 Tokens per Sec: 562.987976\n",
            "Epoch Step: 1 Loss: 0.000414 Tokens per Sec: 538.149536\n",
            "Epoch Step: 1 Loss: 0.027250 Tokens per Sec: 494.486267\n",
            "Epoch Step: 1 Loss: 0.000744 Tokens per Sec: 515.775146\n",
            "Epoch Step: 1 Loss: 0.042821 Tokens per Sec: 489.728027\n",
            "Epoch Step: 1 Loss: 0.002000 Tokens per Sec: 435.382935\n",
            "Epoch Step: 1 Loss: 0.045549 Tokens per Sec: 520.348999\n",
            "Epoch Step: 1 Loss: 0.000564 Tokens per Sec: 507.768585\n",
            "Epoch Step: 1 Loss: 0.069668 Tokens per Sec: 556.320374\n",
            "Epoch Step: 1 Loss: 0.026392 Tokens per Sec: 530.829651\n",
            "Epoch Step: 1 Loss: 0.083667 Tokens per Sec: 538.946167\n",
            "Epoch Step: 1 Loss: 0.001138 Tokens per Sec: 523.419312\n",
            "Epoch Step: 1 Loss: 0.052907 Tokens per Sec: 482.333008\n",
            "Epoch Step: 1 Loss: 0.007511 Tokens per Sec: 434.146912\n",
            "Epoch Step: 1 Loss: 0.055280 Tokens per Sec: 488.129730\n",
            "Epoch Step: 1 Loss: 0.002438 Tokens per Sec: 435.409088\n",
            "Epoch Step: 1 Loss: 0.031475 Tokens per Sec: 531.170410\n",
            "Epoch Step: 1 Loss: 0.017280 Tokens per Sec: 517.205261\n",
            "Epoch Step: 1 Loss: 0.049842 Tokens per Sec: 568.771057\n",
            "Epoch Step: 1 Loss: 0.000735 Tokens per Sec: 555.990601\n",
            "Epoch Step: 1 Loss: 0.044272 Tokens per Sec: 503.906250\n",
            "Epoch Step: 1 Loss: 0.003007 Tokens per Sec: 515.151245\n",
            "Epoch Step: 1 Loss: 0.091027 Tokens per Sec: 486.773773\n",
            "Epoch Step: 1 Loss: 0.013341 Tokens per Sec: 445.207153\n",
            "Epoch Step: 1 Loss: 0.058939 Tokens per Sec: 494.863495\n",
            "Epoch Step: 1 Loss: 0.025973 Tokens per Sec: 507.042633\n",
            "Epoch Step: 1 Loss: 0.041801 Tokens per Sec: 557.103882\n",
            "Epoch Step: 1 Loss: 0.000299 Tokens per Sec: 527.217957\n",
            "Epoch Step: 1 Loss: 0.023139 Tokens per Sec: 554.726685\n",
            "Epoch Step: 1 Loss: 0.000181 Tokens per Sec: 495.711212\n",
            "Epoch Step: 1 Loss: 0.029030 Tokens per Sec: 484.446136\n",
            "Epoch Step: 1 Loss: 0.001773 Tokens per Sec: 520.011169\n",
            "Epoch Step: 1 Loss: 0.032301 Tokens per Sec: 495.069336\n",
            "Epoch Step: 1 Loss: 0.000387 Tokens per Sec: 417.248749\n",
            "Epoch Step: 1 Loss: 0.021131 Tokens per Sec: 523.501526\n",
            "Epoch Step: 1 Loss: 0.007398 Tokens per Sec: 531.658020\n",
            "Epoch Step: 1 Loss: 0.072538 Tokens per Sec: 539.171997\n",
            "Epoch Step: 1 Loss: 0.016554 Tokens per Sec: 559.916687\n",
            "Epoch Step: 1 Loss: 0.039231 Tokens per Sec: 516.167297\n",
            "Epoch Step: 1 Loss: 0.002044 Tokens per Sec: 554.440125\n",
            "Epoch Step: 1 Loss: 0.049892 Tokens per Sec: 490.729950\n",
            "Epoch Step: 1 Loss: 0.049974 Tokens per Sec: 361.551880\n",
            "Epoch Step: 1 Loss: 0.043659 Tokens per Sec: 515.054626\n",
            "Epoch Step: 1 Loss: 0.006912 Tokens per Sec: 544.974060\n",
            "Epoch Step: 1 Loss: 0.031401 Tokens per Sec: 541.719299\n",
            "Epoch Step: 1 Loss: 0.004611 Tokens per Sec: 526.680542\n",
            "Epoch Step: 1 Loss: 0.046151 Tokens per Sec: 511.235443\n",
            "Epoch Step: 1 Loss: 0.000504 Tokens per Sec: 522.583130\n",
            "Epoch Step: 1 Loss: 0.053334 Tokens per Sec: 491.001892\n",
            "Epoch Step: 1 Loss: 0.000513 Tokens per Sec: 417.706207\n",
            "Epoch Step: 1 Loss: 0.078400 Tokens per Sec: 495.122345\n",
            "Epoch Step: 1 Loss: 0.001804 Tokens per Sec: 531.346558\n",
            "Epoch Step: 1 Loss: 0.033173 Tokens per Sec: 532.754456\n",
            "Epoch Step: 1 Loss: 0.001836 Tokens per Sec: 532.980164\n",
            "Epoch Step: 1 Loss: 0.027699 Tokens per Sec: 555.458130\n",
            "Epoch Step: 1 Loss: 0.004437 Tokens per Sec: 513.812012\n",
            "Epoch Step: 1 Loss: 0.041594 Tokens per Sec: 511.742828\n",
            "Epoch Step: 1 Loss: 0.007951 Tokens per Sec: 518.897278\n",
            "Epoch Step: 1 Loss: 0.019607 Tokens per Sec: 481.836700\n",
            "Epoch Step: 1 Loss: 0.000461 Tokens per Sec: 432.876953\n",
            "Epoch Step: 1 Loss: 0.047900 Tokens per Sec: 482.085571\n",
            "Epoch Step: 1 Loss: 0.003246 Tokens per Sec: 459.221252\n",
            "Epoch Step: 1 Loss: 0.036373 Tokens per Sec: 512.423767\n",
            "Epoch Step: 1 Loss: 0.004842 Tokens per Sec: 529.682373\n",
            "Epoch Step: 1 Loss: 0.033868 Tokens per Sec: 528.629700\n",
            "Epoch Step: 1 Loss: 0.000171 Tokens per Sec: 522.765137\n",
            "Epoch Step: 1 Loss: 0.033366 Tokens per Sec: 529.712646\n",
            "Epoch Step: 1 Loss: 0.002187 Tokens per Sec: 543.702087\n",
            "Epoch Step: 1 Loss: 0.078364 Tokens per Sec: 479.375397\n",
            "Epoch Step: 1 Loss: 0.000300 Tokens per Sec: 495.935425\n",
            "Epoch Step: 1 Loss: 0.060373 Tokens per Sec: 488.620636\n",
            "Epoch Step: 1 Loss: 0.012397 Tokens per Sec: 347.710724\n",
            "Epoch Step: 1 Loss: 0.057424 Tokens per Sec: 487.373383\n",
            "Epoch Step: 1 Loss: 0.014052 Tokens per Sec: 423.341705\n",
            "Epoch Step: 1 Loss: 0.030123 Tokens per Sec: 517.335815\n",
            "Epoch Step: 1 Loss: 0.011550 Tokens per Sec: 499.906525\n",
            "Epoch Step: 1 Loss: 0.089319 Tokens per Sec: 534.977600\n",
            "Epoch Step: 1 Loss: 0.000882 Tokens per Sec: 489.410553\n",
            "Epoch Step: 1 Loss: 0.033414 Tokens per Sec: 551.820374\n",
            "Epoch Step: 1 Loss: 0.000878 Tokens per Sec: 497.592651\n",
            "Epoch Step: 1 Loss: 0.039255 Tokens per Sec: 515.664917\n",
            "Epoch Step: 1 Loss: 0.003278 Tokens per Sec: 503.376953\n",
            "Epoch Step: 1 Loss: 0.034697 Tokens per Sec: 468.583496\n",
            "Epoch Step: 1 Loss: 0.002144 Tokens per Sec: 501.544037\n",
            "Epoch Step: 1 Loss: 0.045595 Tokens per Sec: 465.764069\n",
            "Epoch Step: 1 Loss: 0.005529 Tokens per Sec: 404.274078\n",
            "Epoch Step: 1 Loss: 0.046829 Tokens per Sec: 486.673981\n",
            "Epoch Step: 1 Loss: 0.011028 Tokens per Sec: 422.967163\n",
            "Epoch Step: 1 Loss: 0.040822 Tokens per Sec: 504.337677\n",
            "Epoch Step: 1 Loss: 0.000508 Tokens per Sec: 516.135559\n",
            "Epoch Step: 1 Loss: 0.050077 Tokens per Sec: 548.042786\n",
            "Epoch Step: 1 Loss: 0.000504 Tokens per Sec: 502.139801\n",
            "Epoch Step: 1 Loss: 0.054750 Tokens per Sec: 535.312256\n",
            "Epoch Step: 1 Loss: 0.008439 Tokens per Sec: 528.300903\n",
            "Epoch Step: 1 Loss: 0.032534 Tokens per Sec: 500.588531\n",
            "Epoch Step: 1 Loss: 0.001502 Tokens per Sec: 528.277161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model([1,  3,  2,  5,  4,  6,  7,  8,  9, 10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WW-6-5ufnboH",
        "outputId": "201ac46b-e148-4f31-d8eb-6706a9d0eabc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The source numeric sequence is:\n",
            " tensor([[ 1,  3,  2,  5,  4,  6,  7,  8,  9, 10]])\n",
            "The resulting numeric sequence after 100 epochs of training is:\n",
            " tensor([[ 1,  3,  2,  5,  4,  6,  7,  8,  9, 10]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model([6,  2,  5,  1,  4,  3,  7,  10,  8, 9])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wH0GxruWwewR",
        "outputId": "f0c04e84-705d-4e97-aa05-66518553dc17"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The source numeric sequence is:\n",
            " tensor([[ 6,  2,  5,  1,  4,  3,  7, 10,  8,  9]])\n",
            "The resulting numeric sequence after 100 epochs of training is:\n",
            " tensor([[ 1,  6,  5,  1,  4,  3,  7, 10,  8,  9]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model([10,  2,  8,  6,  9,  3,  7,  1,  5, 4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hPaIKXJ1E17",
        "outputId": "16150eed-9f1d-418d-f036-7929e66323e8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The source numeric sequence is:\n",
            " tensor([[10,  2,  8,  6,  9,  3,  7,  1,  5,  4]])\n",
            "The resulting numeric sequence after 100 epochs of training is:\n",
            " tensor([[ 1, 10,  8,  6,  9,  3,  7,  1,  5,  4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model([[1,2,3,4,5,6,7,8,9,10]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Um-xaPLM_OQF",
        "outputId": "fa74d63c-8a91-425d-b0b6-acd0e78ccc8a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The source numeric sequence is:\n",
            " tensor([[[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]]])\n",
            "The resulting numeric sequence after 100 epochs of training is:\n",
            " tensor([[1, 5, 7, 1, 6, 1, 7, 1, 5, 1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model([[10,9,8,7,6,5,4,3,2,1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec1T1eiG_ujE",
        "outputId": "23b0d42d-7859-45ee-843a-6f6a1cf328f9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The source numeric sequence is:\n",
            " tensor([[[10,  9,  8,  7,  6,  5,  4,  3,  2,  1]]])\n",
            "The resulting numeric sequence after 100 epochs of training is:\n",
            " tensor([[1, 5, 7, 1, 6, 1, 7, 1, 5, 1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model([[1,3,5,7,9,2,4,6,8,10]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mH7Y6tcm_7c1",
        "outputId": "62a7bbae-42c6-401a-f3e6-83191f425d44"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The source numeric sequence is:\n",
            " tensor([[[ 1,  3,  5,  7,  9,  2,  4,  6,  8, 10]]])\n",
            "The resulting numeric sequence after 100 epochs of training is:\n",
            " tensor([[1, 5, 7, 1, 6, 1, 7, 1, 5, 1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model([[2,3,4,2,3,4,2,3,4,5]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MX7dQriAUfw",
        "outputId": "e0707bda-ec6e-4d7d-8b78-2a0cbc5d8069"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The source numeric sequence is:\n",
            " tensor([[[2, 3, 4, 2, 3, 4, 2, 3, 4, 5]]])\n",
            "The resulting numeric sequence after 100 epochs of training is:\n",
            " tensor([[1, 4, 3, 4, 3, 4, 3, 4, 3, 4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model([[1,1,1,1,1,1,1,1,1,1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLze03qyAjtD",
        "outputId": "6f16a6aa-384b-4753-c312-9f031968532d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The source numeric sequence is:\n",
            " tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]])\n",
            "The resulting numeric sequence after 100 epochs of training is:\n",
            " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model([[2,2,2,2,2,2,2,2,2,2]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v91NF1j4Ap6F",
        "outputId": "f621b398-4f65-4572-e2c6-ef8d828e3112"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The source numeric sequence is:\n",
            " tensor([[[2, 2, 2, 2, 2, 2, 2, 2, 2, 2]]])\n",
            "The resulting numeric sequence after 100 epochs of training is:\n",
            " tensor([[1, 2, 2, 2, 2, 2, 2, 2, 2, 2]])\n"
          ]
        }
      ]
    }
  ]
}