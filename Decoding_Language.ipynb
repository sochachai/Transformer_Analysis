{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNZBbEX0t5hlEjDCJ7L2YeI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sochachai/Transformer_Analysis/blob/main/Decoding_Language.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load packages"
      ],
      "metadata": {
        "id": "QadEcdM9HWsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check directory\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8gwuRviI0JD",
        "outputId": "ff305cca-cda6-4911-fa0d-fd9baf6fd9e9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# confirm the assistant py file has been uploaded to the correct directory\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kj093MmNI24A",
        "outputId": "032c3e9f-eae3-4251-ac25-2409b2cea49c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encripted_document.txt\tmy_transformer_utils.py  original_document.txt\t__pycache__  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check Python version\n",
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9lyQC3sJbJs",
        "outputId": "c3be1051-0cb6-46c2-8431-7b72ca0cab0e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install torch\n",
        "!pip install torch==2.2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRb0L4jOJzuY",
        "outputId": "9bff65ec-ed42-4fa0-e98d-08e3065ce548"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.2.0\n",
            "  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.0) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.1+cu121\n",
            "    Uninstalling torch-2.2.1+cu121:\n",
            "      Successfully uninstalled torch-2.2.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.2.0 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.2.0 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 torch-2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check torch version\n",
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AWYB4ZIJnlB",
        "outputId": "0c78a336-93fc-4a17-f8e4-25113304c902"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZL5Plt3mvusS"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "# Issue caused by torch version described below:\n",
        "# the original package should be pyitcast.transformer_utils\n",
        "# the problem is that pyitcast.transformer_utils relies on an old version of torch (1.3.1 should work)\n",
        "# and causes the error \"IndexError: invalid index of a 0-dim tensor. Use `tensor.item()` in Python...\"\n",
        "# but the installation of an old version of torch that match pyitcast.transformer is not trivial\n",
        "# versions of 1.11.0 or after is not compatible with pyitcast.transformer and installation of them will not\n",
        "# solve the issue\n",
        "# To solve this issue:\n",
        "# download the pyitcast.transformer_utils (open by clicking the error message) as a py file\n",
        "# modify the last line of the SimpleLossCompute class from \"return loss.data[0] * norm\"\n",
        "# to \"return loss.data * norm\"\n",
        "\n",
        "from my_transformer_utils import Batch\n",
        "from my_transformer_utils import run_epoch\n",
        "from my_transformer_utils import greedy_decode\n",
        "from my_transformer_utils import get_std_opt # get_std_opt is based on Adam optimizer\n",
        "from my_transformer_utils import LabelSmoothing # offset human label errors to prevent overfitting\n",
        "from my_transformer_utils import SimpleLossCompute # calculate loss after smoothing, use cross_entropy_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct the class of Embeddings and Positional Encoding."
      ],
      "metadata": {
        "id": "UBTCCLP4Hc3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        '''\n",
        "        :param d_model: embedding dimension\n",
        "        :param vocab: size of vocabulary\n",
        "        '''\n",
        "        # Initialization\n",
        "        super(Embeddings, self).__init__()\n",
        "        # Defrine a word embedding object\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        # Instantiate d_model\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        :param x: tensor representing the original text\n",
        "        '''\n",
        "        return self.lut(x) * math.sqrt(self.d_model)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout, max_len = 5000):\n",
        "        '''\n",
        "        :param d_model: dimension of the encoding\n",
        "        :param dropout: dropout rate from 0 to 1\n",
        "        :param max_len: the maximum length of a sentence\n",
        "        '''\n",
        "        # Inherit the initialization of nn.Module\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # Objectify dropout\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Inherit a positional encoder matrix, max_len * d_model\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        # Inherit an absolute position matrix, max_len * 1\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "\n",
        "        # Define the conversion matrix, initialization with gap = 2\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0)/d_model))\n",
        "\n",
        "        # Copy the absolute position matrix to the positional encoder matrix\n",
        "        # by sine and cosine wave according to the parity of column indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) # even indiced columns are imputed by sine\n",
        "        pe[:, 1::2] = torch.cos(position * div_term) # odd indiced columns are imputed by cosine\n",
        "\n",
        "        # Extend pe to 3-dimensional tensor\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # Register pe to a buffer, the buffer is not a parameter of the class\n",
        "        # the buffer will not be updated along with the model update\n",
        "        # but it can be loaded along with the model\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        :param x: Tensor of text\n",
        "        :return: x + the positional encoding\n",
        "        '''\n",
        "        # Shrink the size of pe to save storage\n",
        "        # by converting the second dimension, i.e. the dimension of max_len\n",
        "        # to the size of the sentence len of x, i.e. the second dimension of x\n",
        "        x = x + Variable(self.pe[:,:x.size(1)], requires_grad = False) # False: pe will not be updated\n",
        "        return self.dropout(x)\n",
        "\n",
        "def attention(query, key, value, mask = None, dropout = None):\n",
        "    '''\n",
        "    :param query: vectorized original text\n",
        "    :param key: key words of text\n",
        "    :param value: the original value of key, summarization of query\n",
        "    :param mask: hide words to avoid data leakage\n",
        "    :param dropout: dropout rate of neural network\n",
        "    :return:\n",
        "    '''\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9) # compare each position with 0\n",
        "\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "\n",
        "    return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "def clones(module, N):\n",
        "    '''\n",
        "    :param module: one attention layer\n",
        "    :param N: the number of module\n",
        "    '''\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)]) # deepcopy uses a different memory\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, head, embedding_dim, dropout = 0.1):\n",
        "        '''\n",
        "        :param head: the number of heads\n",
        "        :param embedding_dim: the embedding dimension\n",
        "        :param dropout: default dropout rate set to 0.1\n",
        "        '''\n",
        "        # Inherit the initialization\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        # head must be an integral factor of embedding_dim\n",
        "        assert embedding_dim % head == 0\n",
        "        # each head is assigned with the following dimension\n",
        "        self.d_k = embedding_dim // head # division in the integral domain Z\n",
        "        # substantiate head\n",
        "        self.head = head\n",
        "        # create linear layers, we need 4 of them for Q, K, V and the final connection\n",
        "        self.linears = clones(nn.Linear(embedding_dim, embedding_dim), 4)\n",
        "        # create the attention and dropout rate\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p = dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # We have 4 linears and only zipping the first 3 with Q, K, V, the last linear is not used.\n",
        "        # view(batch_size, -1, self.head, self.d_k).transpose(1,2)\n",
        "        # is not the same with view(batch_size, self.head, -1, self.d_k)\n",
        "        # self.head and self.d_k should be neighboring to get embedding_dim in order for the tensor\n",
        "        # to interpret the relationship between the meaning of words of their positions in a sentence\n",
        "\n",
        "        query, key, value = \\\n",
        "            [model(x).view(batch_size, -1, self.head, self.d_k).transpose(1,2)\n",
        "             for model, x in zip(self.linears, (query, key, value))]\n",
        "\n",
        "        x, self.attn = attention(query, key, value, mask = mask, dropout = self.dropout)\n",
        "\n",
        "        # Reshape x\n",
        "        # must use contiguous method after the transpose before the view method\n",
        "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.head * self.d_k)\n",
        "\n",
        "        # Pass x to the 4th linear layer\n",
        "        return self.linears[-1](x)\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        '''\n",
        "        :param d_model: embedding dimension\n",
        "        :param d_ff: transitional dimension\n",
        "        :param dropout: default dropout set to 0.1\n",
        "        '''\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w1 = nn.Linear(d_model, d_ff)\n",
        "        self.w2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w2(self.dropout(F.relu(self.w1(x))))\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, features, eps = 1e-6):\n",
        "        '''\n",
        "        :param features: embedding dimension\n",
        "        :param eps: avoid zero denominator\n",
        "        '''\n",
        "        super(LayerNorm, self).__init__()\n",
        "        # nn.Parameter formats the variables to intrinsic parameters\n",
        "        # they will be updated along with the model in contrast with buffer\n",
        "        self.a2 = nn.Parameter(torch.ones(features))\n",
        "        self.b2 = nn.Parameter(torch.zeros(features))\n",
        "        # initialization of eps\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        :param x: the output of previous layer\n",
        "        :return: numerical standardized x\n",
        "        '''\n",
        "        mean = x.mean(-1, keepdim = True)\n",
        "        std = x.std(-1, keepdim = True)\n",
        "        return self.a2 * (x - mean) / (std + self.eps) + self.b2\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "    def __init__(self, size, dropout = 0.1):\n",
        "        '''\n",
        "        :param size: embedding size\n",
        "        :param dropout: deactivation of neurons to avoid overfitting\n",
        "        '''\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        '''\n",
        "        :param x: the output of previous layer\n",
        "        :param sublayer: a function, e.g. Multihead_Attention, PositionwiseFeedForward etc.\n",
        "        :return: x plus sublayer functioning on normed x with dropout\n",
        "        '''\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        '''\n",
        "        :param size: embedding dimension\n",
        "        :param self_attn: attention\n",
        "        :param feed_forward: positionwise feed forward\n",
        "        :param dropout: avoid overfitting\n",
        "        '''\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        '''\n",
        "        :param x: output of previous layer\n",
        "        :param mask: tensor mask to prevent data leakage\n",
        "        '''\n",
        "        # input matrix followed by operating function, returns an output matrix\n",
        "        # sublayer(x, function) will return function(x)\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) # the forward function of multihead attention\n",
        "        return self.sublayer[1](x, self.feed_forward) # the forward function of pointwise feedforward\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    # Encoder is a collection of Encoder Layers\n",
        "    def __init__(self, layer, N):\n",
        "        '''\n",
        "        :param layer: one encoder layer\n",
        "        :param N: the number of encoder layers\n",
        "        '''\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers: x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        '''\n",
        "        :param size: embedding dimension\n",
        "        :param self_attn: masked multihead attention\n",
        "        :param src_attn: multihead attention\n",
        "        :param feed_forward: pointwise feed forward\n",
        "        :param dropout: avoid overfitting\n",
        "        '''\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        # 3 sublayer for a decoder layer\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        "\n",
        "    def forward(self, x, memory, source_mask, target_mask):\n",
        "        '''\n",
        "        :param x: output of previous layer\n",
        "        :param memory: result of encoder\n",
        "        :param source_mask: delete unnecessary info to improve model performance\n",
        "        :param target_mask: hide info to prevent data leakage\n",
        "        :return: output tensor\n",
        "        '''\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, target_mask))\n",
        "        # for second layer, Q = x, K = V = m\n",
        "        x = self.sublayer[1](x, lambda x: self.self_attn(x, m, m, source_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    # Decoder is a collection of Decoder Layers\n",
        "    def __init__(self, layer, N):\n",
        "        '''\n",
        "        :param layer: decoder layer\n",
        "        :param N: the number of decoder layers\n",
        "        '''\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x, memory, source_mask, target_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, source_mask, target_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "        '''\n",
        "        :param d_model: embedding dimension\n",
        "        :param vocab_size: the size of the vocabulary\n",
        "        '''\n",
        "        super(Generator, self).__init__()\n",
        "        self.project = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.project(x), dim = -1)\n",
        "\n",
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder, source_embed, target_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = source_embed\n",
        "        self.tgt_embed = target_embed\n",
        "        self.generator = generator\n",
        "\n",
        "    def forward(self, source, target, source_mask, target_mask):\n",
        "        # encoded source used as memory in decode function\n",
        "        return self.decode(self.encode(source, source_mask), source_mask,\n",
        "                           target, target_mask)\n",
        "\n",
        "    def encode(self, source, source_mask):\n",
        "        return self.encoder(self.src_embed(source), source_mask)\n",
        "\n",
        "    def decode(self, memory, source_mask, target, target_mask):\n",
        "        # embedded target as x in the decoder function\n",
        "        return self.decoder(self.tgt_embed(target), memory, source_mask, target_mask)\n",
        "\n",
        "def make_model(source_vocab, target_vocab, N=6,\\\n",
        "               d_model=512, d_ff=2048, head=8, dropout=0.1):\n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(head, d_model)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn),c(ff), dropout), N),\n",
        "        # note the order of vocab_size and d_model;\n",
        "        # nn.Embedding is not the same with Embeddings;\n",
        "        # check Embedding_Encoder.py for more;\n",
        "        nn.Sequential(Embeddings(d_model, source_vocab), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, target_vocab), c(position)),\n",
        "        Generator(d_model, target_vocab)\n",
        "    )\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1: nn.init.xavier_uniform_(p) # initialization: make p uniformly sampled; check xavier_uniform for details\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Task: Test the language model's capability to copy a sequence of numbers\n",
        "      Input a list of numbers, the output should be the identical list of numbers\n",
        "SubTask: Write a data generator to generate sample data for model testing\n",
        "'''\n",
        "\n",
        "# use a function to generate data\n",
        "def data_generator(V, batch_size, num_batch):\n",
        "    '''\n",
        "    :param V: the maximal data value + 1\n",
        "    :param batch_size: sample data size of one round of training after which model parameters are updated\n",
        "    :param num_batch: number of rounds of training\n",
        "    '''\n",
        "    for i in range(num_batch):\n",
        "        # data value from 1 to V-1, with data matrix shape = batch_size times 10\n",
        "        data = torch.from_numpy(np.random.randint(1, V, size = (batch_size, 40)))\n",
        "\n",
        "        # set starting position label\n",
        "        data[:, 0] = 1\n",
        "\n",
        "        # for a copy task source data and target data should be the same\n",
        "        # no gradient calculation involved\n",
        "        source = Variable(data, requires_grad = True)\n",
        "        target = Variable(data, requires_grad = True)\n",
        "        #print(f\"source is {source}\")\n",
        "        #return Batch(source, target)\n",
        "        yield Batch(source, target)\n",
        "\n",
        "def data_generator_letter(batch_size, num_batch, initial_document_index):\n",
        "    '''\n",
        "    :param batch_size: sample data size of one round of training after which model parameters are updated\n",
        "                       how many sentences\n",
        "    :param num_batch: number of rounds of training\n",
        "                      how many text documents\n",
        "    '''\n",
        "    #batch_size = 20\n",
        "    #start_batch_index = 0\n",
        "    #num_batch = 8\n",
        "    #original_file = '/content/original_document'+'/original_document'+'_'+str(i)+'.txt'\n",
        "    #with open(original_file, 'r') as file:\n",
        "    #    original_sentences = file.readlines()\n",
        "    fix_letter_indicator_value = 0\n",
        "    max_sentence_len = 50\n",
        "    #encripted_file = '/content/encripted_document'+'/encripted_document'+'_'+str(i)+'.txt'\n",
        "    #with open(encripted_file, 'r') as file:\n",
        "    #    encripted_sentences = file.readlines()\n",
        "\n",
        "    for i in range(num_batch):\n",
        "        original_file = '/content/original_document' + '/original_document' + '_'\\\n",
        "                          + str(i + initial_document_index) + '.txt'\n",
        "        with open(original_file, 'r') as file:\n",
        "            original_sentences = file.readlines()\n",
        "        encripted_file = '/content/encripted_document' + '/encripted_document' + '_'\\\n",
        "                          + str(i + initial_document_index) + '.txt'\n",
        "        with open(encripted_file, 'r') as file:\n",
        "            encripted_sentences = file.readlines()\n",
        "\n",
        "        # Limit Number of sentences\n",
        "        original_sentences = original_sentences[0:batch_size]\n",
        "        #original_sentences = original_sentences[batch_size * 1 : batch_size * (1+1)]\n",
        "        original_sentences = [sentence.rstrip('\\n').lower() for sentence in original_sentences]\n",
        "\n",
        "        # Limit Number of sentences\n",
        "        encripted_sentences = encripted_sentences[0:batch_size]\n",
        "        #encripted_sentences = encripted_sentences[batch_size * i : batch_size * (i+1)]\n",
        "        encripted_sentences = [sentence.rstrip('\\n').lower() for sentence in encripted_sentences]\n",
        "\n",
        "        def get_numeric(symbol):\n",
        "            if symbol == 'a': return 1\n",
        "            elif symbol == 'b': return 2\n",
        "            elif symbol == 'c': return 3\n",
        "            elif symbol == 'd': return 4\n",
        "            elif symbol == 'e': return 5\n",
        "            elif symbol == 'f': return 6\n",
        "            elif symbol == 'g': return 7\n",
        "            elif symbol == 'h': return 8\n",
        "            elif symbol == 'i': return 9\n",
        "            else: return fix_letter_indicator_value\n",
        "\n",
        "        numeric_original_text = np.empty([1,max_sentence_len])\n",
        "        for sentence_index, one_sentence in enumerate(original_sentences):\n",
        "            numeric_sentence = np.pad([get_numeric(item) for index, item in enumerate(one_sentence[:max_sentence_len])],\\\n",
        "                                      (0, max_sentence_len - len(one_sentence[:max_sentence_len])), 'constant',\\\n",
        "                                      constant_values=(fix_letter_indicator_value, fix_letter_indicator_value))\n",
        "            numeric_original_text = np.vstack((numeric_original_text, numeric_sentence))\n",
        "        numeric_original_text = torch.from_numpy(numeric_original_text[1:,])\n",
        "        # set starting position label\n",
        "        numeric_original_text[:, 0] = 1\n",
        "\n",
        "        numeric_encripted_text = np.empty([1,max_sentence_len])\n",
        "        for sentence_index, one_sentence in enumerate(encripted_sentences):\n",
        "            numeric_sentence = np.pad([get_numeric(item) for index, item in enumerate(one_sentence[:max_sentence_len])],\\\n",
        "                                      (0, max_sentence_len - len(one_sentence[:max_sentence_len])), 'constant',\\\n",
        "                                      constant_values=(fix_letter_indicator_value, fix_letter_indicator_value))\n",
        "            numeric_encripted_text = np.vstack((numeric_encripted_text, numeric_sentence))\n",
        "        numeric_encripted_text = torch.from_numpy(numeric_encripted_text[1:,])\n",
        "        # set starting position label\n",
        "        numeric_encripted_text[:, 0] = 1\n",
        "\n",
        "        # for a copy task source data and target data should be the same\n",
        "        # no gradient calculation involved\n",
        "        source = Variable(numeric_encripted_text, requires_grad = False)\n",
        "        target = Variable(numeric_original_text, requires_grad = False)\n",
        "        source = source.type(torch.int64)\n",
        "        target = target.type(torch.int64)\n",
        "        #print(f\"source is {source}\")\n",
        "        #return Batch(source, target)\n",
        "        yield Batch(source, target)\n",
        "\n",
        "def run(model, loss, epochs=10):\n",
        "    '''\n",
        "    :param model: model\n",
        "    :param loss: loss function\n",
        "    :param epochs: number of rounds of training\n",
        "    '''\n",
        "    for epoch in range(epochs):\n",
        "        # train model, update model parameters\n",
        "        model.train()\n",
        "        run_epoch(data_generator_letter(10, 10, 0), model, loss)\n",
        "        #evaluate model, no parameters update\n",
        "        model.eval()\n",
        "        run_epoch(data_generator_letter(10, 2, 10), model, loss)\n"
      ],
      "metadata": {
        "id": "q5g__pE7CWic"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate a model."
      ],
      "metadata": {
        "id": "KEdHXxGxHELW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate variables\n",
        "V = 10 # maximal digit + 1\n",
        "#batch_size = 20\n",
        "#num_batch = 8\n",
        "\n",
        "# get model\n",
        "model = make_model(V, V, N = 6)\n",
        "\n",
        "# get optimizer\n",
        "model_optimizer = get_std_opt(model)\n",
        "\n",
        "# get smooth criterion\n",
        "criterion = LabelSmoothing(size = V, padding_idx = 0, smoothing = 0.0)\n",
        "\n",
        "# get loss function\n",
        "# model as an EncoderDecoder whose last element is a generator object\n",
        "loss = SimpleLossCompute(model.generator, criterion, model_optimizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_FcF3EqEmy8",
        "outputId": "66de1b7e-eed2-4a0e-b312-4fe54d7c01a2"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "100 epochs of model training."
      ],
      "metadata": {
        "id": "Gf1eGbnNFeB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model training\n",
        "epochs = 100\n",
        "if __name__ == '__main__':\n",
        "    run(model, loss, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLdoV2-vsitL",
        "outputId": "8632b52e-5dfe-4c60-9deb-6a20f5ba082e"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch Step: 1 Loss: 1.539525 Tokens per Sec: 34.232719\n",
            "Epoch Step: 1 Loss: 1.218296 Tokens per Sec: 38.340794\n",
            "Epoch Step: 1 Loss: 1.572527 Tokens per Sec: 40.535507\n",
            "Epoch Step: 1 Loss: 1.164031 Tokens per Sec: 38.860886\n",
            "Epoch Step: 1 Loss: 1.518221 Tokens per Sec: 41.887856\n",
            "Epoch Step: 1 Loss: 1.100966 Tokens per Sec: 38.521812\n",
            "Epoch Step: 1 Loss: 1.318651 Tokens per Sec: 41.727764\n",
            "Epoch Step: 1 Loss: 1.053504 Tokens per Sec: 39.684784\n",
            "Epoch Step: 1 Loss: 1.296444 Tokens per Sec: 39.904469\n",
            "Epoch Step: 1 Loss: 1.088313 Tokens per Sec: 42.777382\n",
            "Epoch Step: 1 Loss: 1.400839 Tokens per Sec: 38.195980\n",
            "Epoch Step: 1 Loss: 0.978581 Tokens per Sec: 45.363163\n",
            "Epoch Step: 1 Loss: 1.304712 Tokens per Sec: 36.200066\n",
            "Epoch Step: 1 Loss: 0.865504 Tokens per Sec: 45.312492\n",
            "Epoch Step: 1 Loss: 1.420122 Tokens per Sec: 36.555569\n",
            "Epoch Step: 1 Loss: 0.902712 Tokens per Sec: 45.243916\n",
            "Epoch Step: 1 Loss: 1.365049 Tokens per Sec: 36.933239\n",
            "Epoch Step: 1 Loss: 0.856336 Tokens per Sec: 44.613811\n",
            "Epoch Step: 1 Loss: 1.483157 Tokens per Sec: 36.279194\n",
            "Epoch Step: 1 Loss: 0.950162 Tokens per Sec: 44.961906\n",
            "Epoch Step: 1 Loss: 1.471833 Tokens per Sec: 36.615860\n",
            "Epoch Step: 1 Loss: 0.938604 Tokens per Sec: 45.048668\n",
            "Epoch Step: 1 Loss: 1.428570 Tokens per Sec: 36.730343\n",
            "Epoch Step: 1 Loss: 0.896883 Tokens per Sec: 41.520557\n",
            "Epoch Step: 1 Loss: 1.437842 Tokens per Sec: 39.374832\n",
            "Epoch Step: 1 Loss: 0.822906 Tokens per Sec: 39.459515\n",
            "Epoch Step: 1 Loss: 1.168491 Tokens per Sec: 41.832909\n",
            "Epoch Step: 1 Loss: 0.749165 Tokens per Sec: 38.912979\n",
            "Epoch Step: 1 Loss: 1.294217 Tokens per Sec: 41.770409\n",
            "Epoch Step: 1 Loss: 0.794706 Tokens per Sec: 39.086952\n",
            "Epoch Step: 1 Loss: 1.135285 Tokens per Sec: 42.174450\n",
            "Epoch Step: 1 Loss: 0.796607 Tokens per Sec: 39.349377\n",
            "Epoch Step: 1 Loss: 1.266735 Tokens per Sec: 42.114048\n",
            "Epoch Step: 1 Loss: 0.749691 Tokens per Sec: 39.229465\n",
            "Epoch Step: 1 Loss: 1.190255 Tokens per Sec: 42.427757\n",
            "Epoch Step: 1 Loss: 0.675677 Tokens per Sec: 38.756134\n",
            "Epoch Step: 1 Loss: 1.095749 Tokens per Sec: 41.771484\n",
            "Epoch Step: 1 Loss: 0.581411 Tokens per Sec: 39.363277\n",
            "Epoch Step: 1 Loss: 1.238063 Tokens per Sec: 42.444668\n",
            "Epoch Step: 1 Loss: 0.612890 Tokens per Sec: 41.274696\n",
            "Epoch Step: 1 Loss: 1.116649 Tokens per Sec: 40.791122\n",
            "Epoch Step: 1 Loss: 0.645403 Tokens per Sec: 42.652817\n",
            "Epoch Step: 1 Loss: 1.064335 Tokens per Sec: 37.721725\n",
            "Epoch Step: 1 Loss: 0.567981 Tokens per Sec: 44.818935\n",
            "Epoch Step: 1 Loss: 1.100960 Tokens per Sec: 36.299316\n",
            "Epoch Step: 1 Loss: 0.542106 Tokens per Sec: 45.400852\n",
            "Epoch Step: 1 Loss: 1.107355 Tokens per Sec: 36.611298\n",
            "Epoch Step: 1 Loss: 0.452966 Tokens per Sec: 39.587906\n",
            "Epoch Step: 1 Loss: 1.120231 Tokens per Sec: 36.523682\n",
            "Epoch Step: 1 Loss: 0.538877 Tokens per Sec: 44.996468\n",
            "Epoch Step: 1 Loss: 1.057553 Tokens per Sec: 37.792511\n",
            "Epoch Step: 1 Loss: 0.608450 Tokens per Sec: 40.244419\n",
            "Epoch Step: 1 Loss: 1.087479 Tokens per Sec: 41.746758\n",
            "Epoch Step: 1 Loss: 0.556477 Tokens per Sec: 40.096615\n",
            "Epoch Step: 1 Loss: 1.006478 Tokens per Sec: 42.488117\n",
            "Epoch Step: 1 Loss: 0.437577 Tokens per Sec: 40.606560\n",
            "Epoch Step: 1 Loss: 0.829671 Tokens per Sec: 42.969696\n",
            "Epoch Step: 1 Loss: 0.347165 Tokens per Sec: 40.599148\n",
            "Epoch Step: 1 Loss: 0.787399 Tokens per Sec: 42.839474\n",
            "Epoch Step: 1 Loss: 0.423665 Tokens per Sec: 39.927414\n",
            "Epoch Step: 1 Loss: 0.630592 Tokens per Sec: 42.310154\n",
            "Epoch Step: 1 Loss: 0.379684 Tokens per Sec: 40.211346\n",
            "Epoch Step: 1 Loss: 0.995934 Tokens per Sec: 42.599838\n",
            "Epoch Step: 1 Loss: 0.922980 Tokens per Sec: 43.219688\n",
            "Epoch Step: 1 Loss: 0.909286 Tokens per Sec: 39.222092\n",
            "Epoch Step: 1 Loss: 0.607407 Tokens per Sec: 45.749630\n",
            "Epoch Step: 1 Loss: 1.076386 Tokens per Sec: 37.139713\n",
            "Epoch Step: 1 Loss: 0.483341 Tokens per Sec: 45.955746\n",
            "Epoch Step: 1 Loss: 0.925288 Tokens per Sec: 37.477203\n",
            "Epoch Step: 1 Loss: 0.386943 Tokens per Sec: 46.302025\n",
            "Epoch Step: 1 Loss: 0.847067 Tokens per Sec: 38.131035\n",
            "Epoch Step: 1 Loss: 0.575296 Tokens per Sec: 46.001801\n",
            "Epoch Step: 1 Loss: 0.932957 Tokens per Sec: 37.464943\n",
            "Epoch Step: 1 Loss: 0.520884 Tokens per Sec: 45.735123\n",
            "Epoch Step: 1 Loss: 0.778227 Tokens per Sec: 37.319038\n",
            "Epoch Step: 1 Loss: 0.417078 Tokens per Sec: 44.299946\n",
            "Epoch Step: 1 Loss: 0.693991 Tokens per Sec: 38.626060\n",
            "Epoch Step: 1 Loss: 0.376007 Tokens per Sec: 41.987827\n",
            "Epoch Step: 1 Loss: 0.781385 Tokens per Sec: 40.970963\n",
            "Epoch Step: 1 Loss: 0.495560 Tokens per Sec: 39.504475\n",
            "Epoch Step: 1 Loss: 0.714315 Tokens per Sec: 42.201996\n",
            "Epoch Step: 1 Loss: 0.362453 Tokens per Sec: 40.123528\n",
            "Epoch Step: 1 Loss: 0.885266 Tokens per Sec: 42.147591\n",
            "Epoch Step: 1 Loss: 0.282713 Tokens per Sec: 40.304214\n",
            "Epoch Step: 1 Loss: 0.717231 Tokens per Sec: 42.912106\n",
            "Epoch Step: 1 Loss: 0.373985 Tokens per Sec: 39.774635\n",
            "Epoch Step: 1 Loss: 0.764492 Tokens per Sec: 41.801239\n",
            "Epoch Step: 1 Loss: 0.430945 Tokens per Sec: 40.409222\n",
            "Epoch Step: 1 Loss: 0.599431 Tokens per Sec: 41.688007\n",
            "Epoch Step: 1 Loss: 0.465911 Tokens per Sec: 44.540012\n",
            "Epoch Step: 1 Loss: 0.916317 Tokens per Sec: 37.919498\n",
            "Epoch Step: 1 Loss: 0.505434 Tokens per Sec: 44.564041\n",
            "Epoch Step: 1 Loss: 0.691884 Tokens per Sec: 35.682308\n",
            "Epoch Step: 1 Loss: 0.446105 Tokens per Sec: 43.977345\n",
            "Epoch Step: 1 Loss: 0.536084 Tokens per Sec: 36.980911\n",
            "Epoch Step: 1 Loss: 0.255686 Tokens per Sec: 42.722683\n",
            "Epoch Step: 1 Loss: 0.488841 Tokens per Sec: 40.199696\n",
            "Epoch Step: 1 Loss: 0.385411 Tokens per Sec: 39.718098\n",
            "Epoch Step: 1 Loss: 0.570694 Tokens per Sec: 42.100857\n",
            "Epoch Step: 1 Loss: 0.405631 Tokens per Sec: 39.900013\n",
            "Epoch Step: 1 Loss: 0.876393 Tokens per Sec: 41.890152\n",
            "Epoch Step: 1 Loss: 0.292040 Tokens per Sec: 39.326908\n",
            "Epoch Step: 1 Loss: 0.605062 Tokens per Sec: 41.578213\n",
            "Epoch Step: 1 Loss: 0.413461 Tokens per Sec: 39.336468\n",
            "Epoch Step: 1 Loss: 0.417552 Tokens per Sec: 37.830276\n",
            "Epoch Step: 1 Loss: 0.201947 Tokens per Sec: 44.895313\n",
            "Epoch Step: 1 Loss: 0.596876 Tokens per Sec: 37.476353\n",
            "Epoch Step: 1 Loss: 0.307509 Tokens per Sec: 46.088314\n",
            "Epoch Step: 1 Loss: 0.651257 Tokens per Sec: 37.830643\n",
            "Epoch Step: 1 Loss: 0.220449 Tokens per Sec: 45.696957\n",
            "Epoch Step: 1 Loss: 0.620286 Tokens per Sec: 37.201424\n",
            "Epoch Step: 1 Loss: 0.196269 Tokens per Sec: 45.734524\n",
            "Epoch Step: 1 Loss: 0.537022 Tokens per Sec: 37.761261\n",
            "Epoch Step: 1 Loss: 0.215324 Tokens per Sec: 45.470692\n",
            "Epoch Step: 1 Loss: 0.454597 Tokens per Sec: 38.445881\n",
            "Epoch Step: 1 Loss: 0.350646 Tokens per Sec: 39.361309\n",
            "Epoch Step: 1 Loss: 0.627270 Tokens per Sec: 41.991890\n",
            "Epoch Step: 1 Loss: 0.351714 Tokens per Sec: 39.762329\n",
            "Epoch Step: 1 Loss: 0.635132 Tokens per Sec: 42.223221\n",
            "Epoch Step: 1 Loss: 0.190009 Tokens per Sec: 40.729042\n",
            "Epoch Step: 1 Loss: 0.545915 Tokens per Sec: 42.749126\n",
            "Epoch Step: 1 Loss: 0.306288 Tokens per Sec: 40.195965\n",
            "Epoch Step: 1 Loss: 0.356166 Tokens per Sec: 42.471062\n",
            "Epoch Step: 1 Loss: 0.178359 Tokens per Sec: 39.592903\n",
            "Epoch Step: 1 Loss: 0.398207 Tokens per Sec: 42.922623\n",
            "Epoch Step: 1 Loss: 0.336627 Tokens per Sec: 40.308136\n",
            "Epoch Step: 1 Loss: 0.301551 Tokens per Sec: 43.052570\n",
            "Epoch Step: 1 Loss: 0.340797 Tokens per Sec: 41.861691\n",
            "Epoch Step: 1 Loss: 0.410912 Tokens per Sec: 42.057117\n",
            "Epoch Step: 1 Loss: 0.204743 Tokens per Sec: 43.256207\n",
            "Epoch Step: 1 Loss: 0.307178 Tokens per Sec: 39.624992\n",
            "Epoch Step: 1 Loss: 0.206817 Tokens per Sec: 45.538181\n",
            "Epoch Step: 1 Loss: 0.276958 Tokens per Sec: 38.560806\n",
            "Epoch Step: 1 Loss: 0.223701 Tokens per Sec: 46.065796\n",
            "Epoch Step: 1 Loss: 0.448078 Tokens per Sec: 37.775894\n",
            "Epoch Step: 1 Loss: 0.239618 Tokens per Sec: 46.435970\n",
            "Epoch Step: 1 Loss: 0.270045 Tokens per Sec: 37.543976\n",
            "Epoch Step: 1 Loss: 0.200874 Tokens per Sec: 45.673424\n",
            "Epoch Step: 1 Loss: 0.391593 Tokens per Sec: 37.879997\n",
            "Epoch Step: 1 Loss: 0.156343 Tokens per Sec: 46.096909\n",
            "Epoch Step: 1 Loss: 0.423794 Tokens per Sec: 37.758236\n",
            "Epoch Step: 1 Loss: 0.198363 Tokens per Sec: 45.693443\n",
            "Epoch Step: 1 Loss: 0.334974 Tokens per Sec: 37.209373\n",
            "Epoch Step: 1 Loss: 0.298776 Tokens per Sec: 45.733803\n",
            "Epoch Step: 1 Loss: 0.392571 Tokens per Sec: 37.892220\n",
            "Epoch Step: 1 Loss: 0.134835 Tokens per Sec: 43.087738\n",
            "Epoch Step: 1 Loss: 0.354125 Tokens per Sec: 40.368942\n",
            "Epoch Step: 1 Loss: 0.081760 Tokens per Sec: 40.141224\n",
            "Epoch Step: 1 Loss: 0.478458 Tokens per Sec: 42.158772\n",
            "Epoch Step: 1 Loss: 0.152623 Tokens per Sec: 40.111500\n",
            "Epoch Step: 1 Loss: 0.333406 Tokens per Sec: 42.519150\n",
            "Epoch Step: 1 Loss: 0.204076 Tokens per Sec: 40.936764\n",
            "Epoch Step: 1 Loss: 0.325574 Tokens per Sec: 42.825882\n",
            "Epoch Step: 1 Loss: 0.231439 Tokens per Sec: 40.651459\n",
            "Epoch Step: 1 Loss: 0.223843 Tokens per Sec: 42.971237\n",
            "Epoch Step: 1 Loss: 0.230054 Tokens per Sec: 40.195831\n",
            "Epoch Step: 1 Loss: 0.163109 Tokens per Sec: 42.661945\n",
            "Epoch Step: 1 Loss: 0.283321 Tokens per Sec: 40.354515\n",
            "Epoch Step: 1 Loss: 0.375343 Tokens per Sec: 43.163765\n",
            "Epoch Step: 1 Loss: 0.222966 Tokens per Sec: 42.227386\n",
            "Epoch Step: 1 Loss: 0.275642 Tokens per Sec: 41.133438\n",
            "Epoch Step: 1 Loss: 0.170041 Tokens per Sec: 44.849663\n",
            "Epoch Step: 1 Loss: 0.219899 Tokens per Sec: 38.027321\n",
            "Epoch Step: 1 Loss: 0.188161 Tokens per Sec: 46.005516\n",
            "Epoch Step: 1 Loss: 0.299248 Tokens per Sec: 37.321636\n",
            "Epoch Step: 1 Loss: 0.136283 Tokens per Sec: 46.480881\n",
            "Epoch Step: 1 Loss: 0.286533 Tokens per Sec: 37.939060\n",
            "Epoch Step: 1 Loss: 0.166404 Tokens per Sec: 45.947441\n",
            "Epoch Step: 1 Loss: 0.237400 Tokens per Sec: 37.495022\n",
            "Epoch Step: 1 Loss: 0.106507 Tokens per Sec: 46.246407\n",
            "Epoch Step: 1 Loss: 0.147473 Tokens per Sec: 37.434784\n",
            "Epoch Step: 1 Loss: 0.102737 Tokens per Sec: 46.229240\n",
            "Epoch Step: 1 Loss: 0.266143 Tokens per Sec: 37.532539\n",
            "Epoch Step: 1 Loss: 0.112797 Tokens per Sec: 43.896889\n",
            "Epoch Step: 1 Loss: 0.149513 Tokens per Sec: 39.826572\n",
            "Epoch Step: 1 Loss: 0.137259 Tokens per Sec: 40.069782\n",
            "Epoch Step: 1 Loss: 0.211216 Tokens per Sec: 41.939911\n",
            "Epoch Step: 1 Loss: 0.082345 Tokens per Sec: 40.141308\n",
            "Epoch Step: 1 Loss: 0.160451 Tokens per Sec: 42.133736\n",
            "Epoch Step: 1 Loss: 0.039030 Tokens per Sec: 40.391228\n",
            "Epoch Step: 1 Loss: 0.201239 Tokens per Sec: 42.524384\n",
            "Epoch Step: 1 Loss: 0.066617 Tokens per Sec: 40.606018\n",
            "Epoch Step: 1 Loss: 0.084805 Tokens per Sec: 42.829449\n",
            "Epoch Step: 1 Loss: 0.041670 Tokens per Sec: 39.872200\n",
            "Epoch Step: 1 Loss: 0.100631 Tokens per Sec: 42.841064\n",
            "Epoch Step: 1 Loss: 0.045068 Tokens per Sec: 40.275299\n",
            "Epoch Step: 1 Loss: 0.197117 Tokens per Sec: 42.486553\n",
            "Epoch Step: 1 Loss: 0.091817 Tokens per Sec: 42.263004\n",
            "Epoch Step: 1 Loss: 0.108303 Tokens per Sec: 40.715473\n",
            "Epoch Step: 1 Loss: 0.063305 Tokens per Sec: 45.130325\n",
            "Epoch Step: 1 Loss: 0.172012 Tokens per Sec: 37.239624\n",
            "Epoch Step: 1 Loss: 0.098496 Tokens per Sec: 45.676327\n",
            "Epoch Step: 1 Loss: 0.154029 Tokens per Sec: 37.311916\n",
            "Epoch Step: 1 Loss: 0.183327 Tokens per Sec: 45.833508\n",
            "Epoch Step: 1 Loss: 0.295083 Tokens per Sec: 37.632420\n",
            "Epoch Step: 1 Loss: 0.081135 Tokens per Sec: 45.771446\n",
            "Epoch Step: 1 Loss: 0.313417 Tokens per Sec: 37.253616\n",
            "Epoch Step: 1 Loss: 0.064433 Tokens per Sec: 45.827694\n",
            "Epoch Step: 1 Loss: 0.208732 Tokens per Sec: 37.453491\n",
            "Epoch Step: 1 Loss: 0.161573 Tokens per Sec: 44.946209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model's capibility of copying the original sequence with greedy decode after 10 epochs of training."
      ],
      "metadata": {
        "id": "HpkNeAqqr4Yw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(an_integer_sequence_of_size_5):\n",
        "    # greedy decode\n",
        "    # enter evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # get source input\n",
        "    source = Variable(torch.LongTensor([an_integer_sequence_of_size_5]))\n",
        "\n",
        "    # get source mask\n",
        "    # 1 for no masking\n",
        "    source_mask = Variable(torch.ones(1, 1, 8))\n",
        "\n",
        "    # get result\n",
        "    result = greedy_decode(model, source, source_mask, max_len=20, start_symbol=1)\n",
        "    print(f\"The source numeric sequence is:\\n {source}\")\n",
        "    print(f\"The resulting numeric sequence after {epochs} epochs of training is:\\n {result}\")\n",
        "\n",
        "test_model([2,2,2,2,2,2,2,2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cfZ2TBzryq5",
        "outputId": "8e2f5757-07f0-4db8-a0a3-5bc2ff4a9c25"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The source numeric sequence is:\n",
            " tensor([[2, 2, 2, 2, 2, 2, 2, 2]])\n",
            "The resulting numeric sequence after 10 epochs of training is:\n",
            " tensor([[1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def en_to_num(letter):\n",
        "    if letter == 'a': return 1\n",
        "    elif letter == 'b': return 2\n",
        "    elif letter == 'c': return 3\n",
        "    elif letter == 'd': return 4\n",
        "    elif letter == 'e': return 5\n",
        "    elif letter == 'f': return 6\n",
        "    elif letter == 'g': return 7\n",
        "    elif letter == 'h': return 8\n",
        "    elif letter == 'i': return 9\n",
        "    else: return 0\n",
        "\n",
        "def num_to_en(letter):\n",
        "    if letter == 1: return 'a'\n",
        "    elif letter == 2: return 'b'\n",
        "    elif letter == 3: return 'c'\n",
        "    elif letter == 4: return 'd'\n",
        "    elif letter == 5: return 'e'\n",
        "    elif letter == 6: return 'f'\n",
        "    elif letter == 7: return 'g'\n",
        "    elif letter == 8: return 'h'\n",
        "    elif letter == 9: return 'i'\n",
        "    else: return letter\n",
        "\n",
        "def number_to_text(numbers, input_text):\n",
        "    text = ''\n",
        "    for index, item in enumerate(zip(numbers, input_text)):\n",
        "        if item[0] == 0:\n",
        "            text = text + item[1]\n",
        "        else:\n",
        "            text = text + str(num_to_en(item[0]))\n",
        "    return text\n",
        "\n",
        "model.eval()\n",
        "input_text = 'boi'\n",
        "#print([en_to_num(letter) for index, letter in enumerate(input_text)])\n",
        "\n",
        "# get source input\n",
        "source = Variable(torch.LongTensor([[en_to_num(letter) for index, letter in enumerate(input_text)]]))\n",
        "#print(source)\n",
        "\n",
        "# get source mask\n",
        "# 1 for no masking\n",
        "source_mask = Variable(torch.ones(1, 1, len(input_text)))\n",
        "\n",
        "# get result\n",
        "result = greedy_decode(model, source, source_mask, max_len=20, start_symbol=1)\n",
        "output_text = number_to_text(result[0].tolist(), input_text)\n",
        "print(result)\n",
        "print(f\"The input text is: {input_text}\")\n",
        "print(f\"The output text is: {output_text}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_O1N9XrGzgk",
        "outputId": "08f671fc-1f41-44a8-81ee-5ffb14128467"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 9, 4, 1, 9, 4, 4, 4, 1, 9, 7, 9, 4, 5, 4, 5, 4, 5, 1, 9]])\n",
            "The input text is: boi\n",
            "The output text is: aid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model's capibility of copying the original sequence with greedy decode after an extra of 20 epochs of training."
      ],
      "metadata": {
        "id": "tEXOpUlSGGiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model training\n",
        "epochs = 20\n",
        "if __name__ == '__main__':\n",
        "    run(model, loss, epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5SwQh1vGDRJ",
        "outputId": "7b9fb8a5-823e-4884-9321-127931aae86a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch Step: 1 Loss: 1.239350 Tokens per Sec: 162.733932\n",
            "Epoch Step: 1 Loss: 0.908190 Tokens per Sec: 171.653259\n",
            "Epoch Step: 1 Loss: 0.996257 Tokens per Sec: 181.329926\n",
            "Epoch Step: 1 Loss: 0.741328 Tokens per Sec: 148.147736\n",
            "Epoch Step: 1 Loss: 0.963049 Tokens per Sec: 166.191589\n",
            "Epoch Step: 1 Loss: 0.628003 Tokens per Sec: 167.297852\n",
            "Epoch Step: 1 Loss: 0.933253 Tokens per Sec: 169.863144\n",
            "Epoch Step: 1 Loss: 0.629011 Tokens per Sec: 151.229950\n",
            "Epoch Step: 1 Loss: 0.831857 Tokens per Sec: 168.031021\n",
            "Epoch Step: 1 Loss: 0.548762 Tokens per Sec: 174.622253\n",
            "Epoch Step: 1 Loss: 0.777321 Tokens per Sec: 168.225006\n",
            "Epoch Step: 1 Loss: 0.414798 Tokens per Sec: 180.782944\n",
            "Epoch Step: 1 Loss: 0.658169 Tokens per Sec: 177.783142\n",
            "Epoch Step: 1 Loss: 0.373356 Tokens per Sec: 158.143326\n",
            "Epoch Step: 1 Loss: 0.620513 Tokens per Sec: 173.372635\n",
            "Epoch Step: 1 Loss: 0.250569 Tokens per Sec: 157.542694\n",
            "Epoch Step: 1 Loss: 0.555389 Tokens per Sec: 172.896469\n",
            "Epoch Step: 1 Loss: 0.188450 Tokens per Sec: 173.881210\n",
            "Epoch Step: 1 Loss: 0.484835 Tokens per Sec: 174.819748\n",
            "Epoch Step: 1 Loss: 0.093754 Tokens per Sec: 178.385208\n",
            "Epoch Step: 1 Loss: 0.387236 Tokens per Sec: 178.816589\n",
            "Epoch Step: 1 Loss: 0.078070 Tokens per Sec: 178.423111\n",
            "Epoch Step: 1 Loss: 0.312203 Tokens per Sec: 186.147873\n",
            "Epoch Step: 1 Loss: 0.089062 Tokens per Sec: 178.800583\n",
            "Epoch Step: 1 Loss: 0.246656 Tokens per Sec: 182.225616\n",
            "Epoch Step: 1 Loss: 0.042359 Tokens per Sec: 156.204254\n",
            "Epoch Step: 1 Loss: 0.201132 Tokens per Sec: 174.777985\n",
            "Epoch Step: 1 Loss: 0.034264 Tokens per Sec: 160.557770\n",
            "Epoch Step: 1 Loss: 0.163383 Tokens per Sec: 174.672195\n",
            "Epoch Step: 1 Loss: 0.019358 Tokens per Sec: 157.246719\n",
            "Epoch Step: 1 Loss: 0.235335 Tokens per Sec: 175.628494\n",
            "Epoch Step: 1 Loss: 0.019713 Tokens per Sec: 171.628952\n",
            "Epoch Step: 1 Loss: 0.193573 Tokens per Sec: 179.428970\n",
            "Epoch Step: 1 Loss: 0.021383 Tokens per Sec: 176.016129\n",
            "Epoch Step: 1 Loss: 0.148571 Tokens per Sec: 175.526886\n",
            "Epoch Step: 1 Loss: 0.024643 Tokens per Sec: 179.697052\n",
            "Epoch Step: 1 Loss: 0.083639 Tokens per Sec: 172.602295\n",
            "Epoch Step: 1 Loss: 0.021374 Tokens per Sec: 179.665573\n",
            "Epoch Step: 1 Loss: 0.081713 Tokens per Sec: 180.596008\n",
            "Epoch Step: 1 Loss: 0.011070 Tokens per Sec: 173.865677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model([1,  3,  2,  5,  4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WW-6-5ufnboH",
        "outputId": "54ce514c-b745-4066-e8b9-e4b5b8fcf881"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The source numeric sequence is:\n",
            " tensor([[1, 3, 2, 5, 4]])\n",
            "The resulting numeric sequence after 20 epochs of training is:\n",
            " tensor([[1, 3, 2, 5, 5]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model([2,  5,  1,  4,  3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wH0GxruWwewR",
        "outputId": "dd93a4d5-9029-4407-fdea-aba281d1baaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The source numeric sequence is:\n",
            " tensor([[2, 5, 1, 4, 3]])\n",
            "The resulting numeric sequence after 20 epochs of training is:\n",
            " tensor([[1, 5, 1, 4, 3]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model([2,  3,  1,  5, 4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hPaIKXJ1E17",
        "outputId": "dafa9b26-7227-4906-d449-4458c2062ff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The source numeric sequence is:\n",
            " tensor([[2, 3, 1, 5, 4]])\n",
            "The resulting numeric sequence after 20 epochs of training is:\n",
            " tensor([[1, 3, 1, 5, 5]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model([[1,2,3,4,5]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Um-xaPLM_OQF",
        "outputId": "ec268e12-8271-45a6-b492-8aed5c56373a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The source numeric sequence is:\n",
            " tensor([[[1, 2, 3, 4, 5]]])\n",
            "The resulting numeric sequence after 20 epochs of training is:\n",
            " tensor([[1, 2, 4, 3, 5]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model([[5,4,3,2,1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec1T1eiG_ujE",
        "outputId": "93790a23-7631-471b-887d-caa10c1b2179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The source numeric sequence is:\n",
            " tensor([[[5, 4, 3, 2, 1]]])\n",
            "The resulting numeric sequence after 20 epochs of training is:\n",
            " tensor([[1, 2, 4, 3, 5]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model([[1,3,5,2,4]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mH7Y6tcm_7c1",
        "outputId": "4cc6f5ca-bd5c-4047-ba91-a4ea887669ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The source numeric sequence is:\n",
            " tensor([[[1, 3, 5, 2, 4]]])\n",
            "The resulting numeric sequence after 20 epochs of training is:\n",
            " tensor([[1, 2, 4, 3, 5]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model([[2,3,4,2,3]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MX7dQriAUfw",
        "outputId": "49f0f06c-a97e-4321-a3c6-e9a5f2cf30ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The source numeric sequence is:\n",
            " tensor([[[2, 3, 4, 2, 3]]])\n",
            "The resulting numeric sequence after 20 epochs of training is:\n",
            " tensor([[1, 2, 3, 2, 3]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model([[1,1,1,1,1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLze03qyAjtD",
        "outputId": "4aa20d71-9a44-4852-9531-1073ad821f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The source numeric sequence is:\n",
            " tensor([[[1, 1, 1, 1, 1]]])\n",
            "The resulting numeric sequence after 20 epochs of training is:\n",
            " tensor([[1, 1, 1, 1, 1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model([[2,2,2,2,2]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v91NF1j4Ap6F",
        "outputId": "bb66c8d2-334d-4560-a6bd-f69fc19ea022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The source numeric sequence is:\n",
            " tensor([[[2, 2, 2, 2, 2]]])\n",
            "The resulting numeric sequence after 20 epochs of training is:\n",
            " tensor([[1, 2, 2, 2, 2]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model([[0,1,2,3,4]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XBisIRhBgsJ",
        "outputId": "84122c48-63d7-48a2-b8bd-d3e4e7b7c5bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The source numeric sequence is:\n",
            " tensor([[[0, 1, 2, 3, 4]]])\n",
            "The resulting numeric sequence after 20 epochs of training is:\n",
            " tensor([[1, 2, 3, 4, 3]])\n"
          ]
        }
      ]
    }
  ]
}